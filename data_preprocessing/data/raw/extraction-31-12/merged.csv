DOI,Document title,Abstract
,,
,,
,,
,,
,,
,,
,LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games,
,Large language models are in-context semantic reasoners rather than symbolic reasoners,
,,
,Robots that ask for help: Uncertainty alignment for large language model planners,
,,
,,
,Leveraging commonsense knowledge from large language models for task and motion planning,
,,
,,
,,
,,
,,
,Tptu: Task planning and tool usage of large language model-based ai agents,
,Planning with large language models via corrective re-prompting,
,Sayplan: Grounding large language models using 3d scene graphs for scalable task planning,
,,
,,
,Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach,
,,
,,
,Halo: Estimation and reduction of hallucinations in open-source weak large language models,
,,
,,
,,
,,
,,
,,
,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,"Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5. © 2023 Proceedings of Machine Learning Research. All rights reserved."
,,
,,
,Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,"Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Copyright © 2022 by the author(s)"
,,
,,
,,
,,
,"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL","This paper describes aspects of our plan validation tool, VAL. The tool was initially developed to support the 3rd International Planning Competition, but has subsequently been extended in order to exploit its capabilities in plan validation and development. In particular, the tool has been extended to include advanced features of PDDL2.1 which have proved important in mixed-initiative planning in a space operations project. Amongst these features, treatment of continuous effects is the most significant, with important effects on the semantic interpretation of plans. The tool has also been extended to keep abreast of developments in PDDL, providing critical support to participants and organisers of the 4th IPC. © 2004 IEEE."
,,
,REFLECT: Summarizing robot experiences for failure explanation and correction,
,,
,,
,,
,,
,,
,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's “hands and eyes, ” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website, video, and open source can be found at say-can.github.io. © 2023 Proceedings of Machine Learning Research. All rights reserved."
,,
,,
,,
,Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks,
,Pddl planning with pretrained large language models,
,,
,,
,,
,,
,There and Back Again: Extracting Formal Domains for Controllable Neurosymbolic Story Authoring,"Story generators using language models offer the automatic production of highly fluent narrative content, but they are hard to control and understand, seizing creative tasks that many authors wish to perform themselves. On the other hand, planning-based story generators are highly controllable and easily understood but require story domains that must be laboriously crafted; further, they lack the capacity for fluent language generation. In this paper, we explore hybrid approaches that aim to bridge the gap between language models and narrative planners. First, we demonstrate that language models can be used to author narrative planning domains from natural language stories with minimal human intervention. Second, we explore the reverse, demonstrating that we can use logical story domains and plans to produce stories that respect the narrative commitments of the planner. In doing so, we aim to build a foundation for human-centric authoring tools that facilitate novel creative experiences. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
,PDDL - The planning domain definition language,
,,
,Lpg: A planner based on local search for planning graphs with action costs,
,,
,,
,,
,,
,,
,Voxposer: Composable 3d value maps for robotic manipulation with language models,
,,
,,
,,
,,
,Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface,
,,
,,
,,
,,
,,
,,
,,
,,
,Reasoning with language model is planning with world model,
,,
,Chameleon: Plug-and-play compositional reasoning with large language models,
,Smart-llm: Smart multi-agent robot task planning using large language models,
,,
,"Language models, agent models, and world models: The LAW for machine reasoning and planning",
,,
,,
,,
,,
,,
,,
,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning. © 2023 Neural information processing systems foundation. All rights reserved."
,Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,"Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67% to 85%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel. © 2023 Neural information processing systems foundation. All rights reserved."
,,
,,
,,
,Eipe-text: Evaluation-guided iterative plan extraction for long-form narrative text generation,
,Case-based reasoning,
,,
,,
,,
,,
,Tree of thoughts: Deliberate problem solving with large language models,
,Creative robot tool use with large language models,
,,
,,
,,
,,
,,
,Integrating common sense and planning with large language models for room tidying,
,,
,,
,Large language models as commonsense knowledge for large-scale task planning,
,,
,,
,On the planning abilities of large language models (A critical investigation with a proposed benchmark),
,,
,,
,"Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
,,
,,
,,
,,
,From word models to world models: Translating from natural language to the probabilistic language of thought,
,,
,,
,,
,,
,,
,,
,LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games,
,Large language models are in-context semantic reasoners rather than symbolic reasoners,
,,
,Robots that ask for help: Uncertainty alignment for large language model planners,
,,
,,
,Leveraging commonsense knowledge from large language models for task and motion planning,
,,
,,
,,
,,
,,
,Tptu: Task planning and tool usage of large language model-based ai agents,
,Planning with large language models via corrective re-prompting,
,Sayplan: Grounding large language models using 3d scene graphs for scalable task planning,
,,
,,
,Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach,
,,
,,
,Halo: Estimation and reduction of hallucinations in open-source weak large language models,
,,
,,
,,
,,
,,
,,
,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,"Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5. © 2023 Proceedings of Machine Learning Research. All rights reserved."
,,
,,
,Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,"Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Copyright © 2022 by the author(s)"
,,
,,
,,
,,
,"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL","This paper describes aspects of our plan validation tool, VAL. The tool was initially developed to support the 3rd International Planning Competition, but has subsequently been extended in order to exploit its capabilities in plan validation and development. In particular, the tool has been extended to include advanced features of PDDL2.1 which have proved important in mixed-initiative planning in a space operations project. Amongst these features, treatment of continuous effects is the most significant, with important effects on the semantic interpretation of plans. The tool has also been extended to keep abreast of developments in PDDL, providing critical support to participants and organisers of the 4th IPC. © 2004 IEEE."
,,
,REFLECT: Summarizing robot experiences for failure explanation and correction,
,,
,,
,,
,,
,,
,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's “hands and eyes, ” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website, video, and open source can be found at say-can.github.io. © 2023 Proceedings of Machine Learning Research. All rights reserved."
,,
,,
,,
,Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks,
,Pddl planning with pretrained large language models,
,,
,,
,,
,,
,There and Back Again: Extracting Formal Domains for Controllable Neurosymbolic Story Authoring,"Story generators using language models offer the automatic production of highly fluent narrative content, but they are hard to control and understand, seizing creative tasks that many authors wish to perform themselves. On the other hand, planning-based story generators are highly controllable and easily understood but require story domains that must be laboriously crafted; further, they lack the capacity for fluent language generation. In this paper, we explore hybrid approaches that aim to bridge the gap between language models and narrative planners. First, we demonstrate that language models can be used to author narrative planning domains from natural language stories with minimal human intervention. Second, we explore the reverse, demonstrating that we can use logical story domains and plans to produce stories that respect the narrative commitments of the planner. In doing so, we aim to build a foundation for human-centric authoring tools that facilitate novel creative experiences. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
,PDDL - The planning domain definition language,
,,
,Lpg: A planner based on local search for planning graphs with action costs,
,,
,,
,,
,,
,,
,Voxposer: Composable 3d value maps for robotic manipulation with language models,
,,
,,
,,
,,
,Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface,
,,
,,
,,
,,
,,
,,
,,
,,
,Reasoning with language model is planning with world model,
,,
,Chameleon: Plug-and-play compositional reasoning with large language models,
,Smart-llm: Smart multi-agent robot task planning using large language models,
,,
,"Language models, agent models, and world models: The LAW for machine reasoning and planning",
,,
,,
,,
,,
,,
,,
,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning. © 2023 Neural information processing systems foundation. All rights reserved."
,Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,"Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67% to 85%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel. © 2023 Neural information processing systems foundation. All rights reserved."
,,
,,
,,
,Eipe-text: Evaluation-guided iterative plan extraction for long-form narrative text generation,
,Case-based reasoning,
,,
,,
,,
,,
,Tree of thoughts: Deliberate problem solving with large language models,
,Creative robot tool use with large language models,
,,
,,
,,
,,
,,
,Integrating common sense and planning with large language models for room tidying,
,,
,,
,Large language models as commonsense knowledge for large-scale task planning,
,,
,,
,On the planning abilities of large language models (A critical investigation with a proposed benchmark),
,,
,,
,"Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
,,
,,
,,
,,
,From word models to world models: Translating from natural language to the probabilistic language of thought,
10.1002/aaai.12036,The third AI summer: AAAI Robert S. Engelmore Memorial Lecture,
10.1002/aaai.12036,The third AI summer: AAAI Robert S. Engelmore Memorial Lecture,
10.1007/978-3-031-19842-7_21,Housekeep: Tidying Virtual Households Using Commonsense Reasoning,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We find that our baseline planner generalizes to some extent when rearranging objects in unknown environments. See our webpage for code, data and more details: https://yashkant.github.io/housekeep/. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG."
10.1007/978-3-031-19842-7_21,Housekeep: Tidying Virtual Households Using Commonsense Reasoning,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We find that our baseline planner generalizes to some extent when rearranging objects in unknown environments. See our webpage for code, data and more details: https://yashkant.github.io/housekeep/. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG."
10.1007/978-3-031-33469-6_23,Evaluation of Pretrained Large Language Models in Embodied Planning Tasks,"Modern pretrained large language models (LLMs) are increasingly being used in zero-shot or few-shot learning modes. Recent years have seen increased interest in applying such models to embodied artificial intelligence and robotics tasks. When given in a natural language, the agent needs to build a plan based on this prompt. The best solutions use LLMs through APIs or models that are not publicly available, making it difficult to reproduce the results. In this paper, we use publicly available LLMs to build a plan for an embodied agent and evaluate them in three modes of operation: 1) the subtask evaluation mode, 2) the full autoregressive plan generation, and 3) the step-by-step autoregressive plan generation. We used two prompt settings: prompt-containing examples of one given task and a mixed prompt with examples of different tasks. Through extensive experiments, we have shown that the subtask evaluation mode, in most cases, outperforms others with a task-specific prompt, whereas the step-by-step autoregressive plan generation posts better performance in the mixed prompt setting. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG."
10.1007/978-3-031-33469-6_23,Evaluation of Pretrained Large Language Models in Embodied Planning Tasks,"Modern pretrained large language models (LLMs) are increasingly being used in zero-shot or few-shot learning modes. Recent years have seen increased interest in applying such models to embodied artificial intelligence and robotics tasks. When given in a natural language, the agent needs to build a plan based on this prompt. The best solutions use LLMs through APIs or models that are not publicly available, making it difficult to reproduce the results. In this paper, we use publicly available LLMs to build a plan for an embodied agent and evaluate them in three modes of operation: 1) the subtask evaluation mode, 2) the full autoregressive plan generation, and 3) the step-by-step autoregressive plan generation. We used two prompt settings: prompt-containing examples of one given task and a mixed prompt with examples of different tasks. Through extensive experiments, we have shown that the subtask evaluation mode, in most cases, outperforms others with a task-specific prompt, whereas the step-by-step autoregressive plan generation posts better performance in the mixed prompt setting. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG."
10.1007/s10514-023-10135-3,ProgPrompt: program generation for situated robot task planning using large language models,"Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website and code at progprompt.github.io. © 2023, The Author(s)."
10.1007/s10514-023-10135-3,ProgPrompt: program generation for situated robot task planning using large language models,"Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website and code at progprompt.github.io. © 2023, The Author(s)."
10.1016/B978-1-55860-856-6.X5000-5,Automated Planning: Theory and Practice,"Automated planning technology now plays a significant role in a variety of demanding applications, ranging from controlling space vehicles and robots to playing the game of bridge. These real-world applications create new opportunities for synergy between theory and practice: observing what works well in practice leads to better theories of planning, and better theories lead to better performance of practical applications. Automated Planning mirrors this dialogue by offering a comprehensive, up-to-date resource on both the theory and practice of automated planning. The book goes well beyond classical planning, to include temporal planning, resource scheduling, planning under uncertainty, and modern techniques for plan generation, such as task decomposition, propositional satisfiability, constraint satisfaction, and model checking. The authors combine over 30 years experience in planning research and development to offer an invaluable text to researchers, professionals, and graduate students. Comprehensively explains paradigms for automated planning. Provides a thorough understanding of theory and planning practice, and how they relate to each other. Presents case studies of applications in space, robotics, CAD/CAM, process control, emergency operations, and games. Provides a thorough understanding of AI planning theory and practice, and how they relate to each other. Covers all the contemporary topics of planning, as well as important practical applications of planning, such as model checking and game playing. Presents case studies and applications in planning engineering, space, robotics, CAD/CAM, process control, emergency operations, and games. Provides lecture notes, examples of programming assignments, pointers to downloadable planning systems and related information online. © 2004 Elsevier Inc. All rights reserved."
10.1016/B978-1-55860-856-6.X5000-5,Automated Planning: Theory and Practice,"Automated planning technology now plays a significant role in a variety of demanding applications, ranging from controlling space vehicles and robots to playing the game of bridge. These real-world applications create new opportunities for synergy between theory and practice: observing what works well in practice leads to better theories of planning, and better theories lead to better performance of practical applications. Automated Planning mirrors this dialogue by offering a comprehensive, up-to-date resource on both the theory and practice of automated planning. The book goes well beyond classical planning, to include temporal planning, resource scheduling, planning under uncertainty, and modern techniques for plan generation, such as task decomposition, propositional satisfiability, constraint satisfaction, and model checking. The authors combine over 30 years experience in planning research and development to offer an invaluable text to researchers, professionals, and graduate students. Comprehensively explains paradigms for automated planning. Provides a thorough understanding of theory and planning practice, and how they relate to each other. Presents case studies of applications in space, robotics, CAD/CAM, process control, emergency operations, and games. Provides a thorough understanding of AI planning theory and practice, and how they relate to each other. Covers all the contemporary topics of planning, as well as important practical applications of planning, such as model checking and game playing. Presents case studies and applications in planning engineering, space, robotics, CAD/CAM, process control, emergency operations, and games. Provides lecture notes, examples of programming assignments, pointers to downloadable planning systems and related information online. © 2004 Elsevier Inc. All rights reserved."
10.1109/ICCV51070.2023.00280,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. © 2023 IEEE."
10.1109/ICCV51070.2023.00280,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. © 2023 IEEE."
10.1109/ICRA48891.2023.10161534,Open-vocabulary Queryable Scene Representations for Real World Planning,"Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io. © 2023 IEEE."
10.1109/ICRA48891.2023.10161534,Open-vocabulary Queryable Scene Representations for Real World Planning,"Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io. © 2023 IEEE."
10.1609/icaps.v34i1.31510,Learning General Policies for Planning through GPT Models,"Transformer-based architectures, such as T5, BERT and GPT, have demonstrated revolutionary capabilities in Natural Language Processing. Several studies showed that deep learning models using these architectures not only possess remarkable linguistic knowledge, but they also exhibit forms of factual knowledge, common sense, and even programming skills. However, the scientific community still debates about their reasoning capabilities, which have been recently tested in the context of automated AI planning; the literature presents mixed results, and the prevailing view is that current transformer-based models may not be adequate for planning. In this paper, we address this challenge differently. We introduce a GPT-based model customised for planning (PLANGPT) to learn a general policy for classical planning by training the model from scratch with a dataset of solved planning instances. Once PLANGPT has been trained for a domain, it can be used to generate a solution plan for an input problem instance in that domain. Our training procedure exploits automated planning knowledge to enhance the performance of the trained model. We build and evaluate our GPT model with several planning domains, and we compare its performance w.r.t. other recent deep learning techniques for generalised planning, demonstrating the effectiveness of the proposed approach. Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
10.1609/icaps.v34i1.31510,Learning General Policies for Planning through GPT Models,"Transformer-based architectures, such as T5, BERT and GPT, have demonstrated revolutionary capabilities in Natural Language Processing. Several studies showed that deep learning models using these architectures not only possess remarkable linguistic knowledge, but they also exhibit forms of factual knowledge, common sense, and even programming skills. However, the scientific community still debates about their reasoning capabilities, which have been recently tested in the context of automated AI planning; the literature presents mixed results, and the prevailing view is that current transformer-based models may not be adequate for planning. In this paper, we address this challenge differently. We introduce a GPT-based model customised for planning (PLANGPT) to learn a general policy for classical planning by training the model from scratch with a dataset of solved planning instances. Once PLANGPT has been trained for a domain, it can be used to generate a solution plan for an input problem instance in that domain. Our training procedure exploits automated planning knowledge to enhance the performance of the trained model. We build and evaluate our GPT model with several planning domains, and we compare its performance w.r.t. other recent deep learning techniques for generalised planning, demonstrating the effectiveness of the proposed approach. Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
10.18653/v1/2023.findings-emnlp.248,LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,"Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, LOGIC-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate LOGIC-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, LOGIC-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that LOGIC-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. © 2023 Association for Computational Linguistics."
10.18653/v1/2023.findings-emnlp.248,LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,"Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, LOGIC-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate LOGIC-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, LOGIC-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that LOGIC-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. © 2023 Association for Computational Linguistics."
10.24963/ijcai.2023/839,Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers,"Plansformer is a novel tool that utilizes a fine-tuned language model based on transformer architecture to generate symbolic plans. Transformers are a type of neural network architecture that have been shown to be highly effective in a range of natural language processing tasks. Unlike traditional planning systems that use heuristic-based search strategies, Plansformer is fine-tuned on specific classical planning domains to generate high-quality plans that are both fluent and feasible. Plansformer takes the domain and problem files as input (in PDDL) and outputs a sequence of actions that can be executed to solve the problem. We demonstrate the effectiveness of Plansformer on a variety of benchmark problems and provide both qualitative and quantitative results obtained during our evaluation, including its limitations. Plansformer has the potential to significantly improve the efficiency and effectiveness of planning in various domains, from logistics and scheduling to natural language processing and human-computer interaction. In addition, we provide public access to Plansformer via a website as well as an API endpoint; this enables other researchers to utilize our tool for planning and execution. The demo video is available at https://youtu.be/1rlctCGsrk. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved."
10.24963/ijcai.2023/839,Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers,"Plansformer is a novel tool that utilizes a fine-tuned language model based on transformer architecture to generate symbolic plans. Transformers are a type of neural network architecture that have been shown to be highly effective in a range of natural language processing tasks. Unlike traditional planning systems that use heuristic-based search strategies, Plansformer is fine-tuned on specific classical planning domains to generate high-quality plans that are both fluent and feasible. Plansformer takes the domain and problem files as input (in PDDL) and outputs a sequence of actions that can be executed to solve the problem. We demonstrate the effectiveness of Plansformer on a variety of benchmark problems and provide both qualitative and quantitative results obtained during our evaluation, including its limitations. Plansformer has the potential to significantly improve the efficiency and effectiveness of planning in various domains, from logistics and scheduling to natural language processing and human-computer interaction. In addition, we provide public access to Plansformer via a website as well as an API endpoint; this enables other researchers to utilize our tool for planning and execution. The demo video is available at https://youtu.be/1rlctCGsrk. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved."
10.3389/frobt.2023.1221739,Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning,"Long-horizon task planning is essential for the development of intelligent assistive and service robots. In this work, we investigate the applicability of a smaller class of large language models (LLMs), specifically GPT-2, in robotic task planning by learning to decompose tasks into subgoal specifications for a planner to execute sequentially. Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark. We compare our approach with classical planning and baseline methods to examine the applicability and generalizability of LLM-based planners. Our findings suggest that the knowledge stored in an LLM can be effectively grounded to perform long-horizon task planning, demonstrating the promising potential for the future application of neuro-symbolic planning methods in robotics. Copyright © 2023 Chalvatzaki, Younes, Nandha, Le, Ribeiro and Gurevych."
10.3389/frobt.2023.1221739,Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning,"Long-horizon task planning is essential for the development of intelligent assistive and service robots. In this work, we investigate the applicability of a smaller class of large language models (LLMs), specifically GPT-2, in robotic task planning by learning to decompose tasks into subgoal specifications for a planner to execute sequentially. Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark. We compare our approach with classical planning and baseline methods to examine the applicability and generalizability of LLM-based planners. Our findings suggest that the knowledge stored in an LLM can be effectively grounded to perform long-horizon task planning, demonstrating the promising potential for the future application of neuro-symbolic planning methods in robotics. Copyright © 2023 Chalvatzaki, Younes, Nandha, Le, Ribeiro and Gurevych."
