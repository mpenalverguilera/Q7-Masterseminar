"Document Title","DOI","Link","Abstract"
"Towards artificial intelligence-enabled autonomous battery prognostics and management","10.1016/j.jechem.2025.09.081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020664652&doi=10.1016%2Fj.jechem.2025.09.081&partnerID=40&md5=5f00f94b3e6775f6f326108f6841aa7e","Reliable and safe operation of batteries is increasingly challenged by diverse operating conditions and stringent demands for system resilience. Artificial intelligence (AI) has emerged as a transformative enabler of battery health management, offering capabilities beyond traditional models. This review provides a structured synthesis of recent progress in AI-enabled diagnostics. Advances in state estimation—including state of health (SOH) and remaining useful life (RUL)—are first examined, with methodological breakthroughs identified across diverse task formulations. The evolution of AI architectures is then traced, from conventional neural networks to attention-based Transformers, physics-informed models, and federated learning, with particular attention to emerging paradigms such as foundation models, neuro-symbolic reasoning, and quantum machine learning that promise improved robustness and interpretability. To bridge laboratory innovation with deployment, a domain-adaptive four-stage data pipeline has emerged as a promising framework for real-world BMS signals—spanning operational segmentation, multi-scale denoising, degradation-aware feature engineering, and structured sample construction—designed to enhance generalization under heterogeneous and noisy conditions. Looking forward, a technological roadmap is outlined that integrates edge AI, digital twins, AIOps, quantum computing, wireless sensing, and self-repair systems. Collectively, these innovations transform batteries from passive energy reservoirs into intelligent cyber-physical agents endowed with perception, autonomous decision-making, and resilient fault response—paving the way toward truly battery-centric autonomous energy systems. © 2025 Science Press"
"Hybrid of representation learning and reinforcement learning for dynamic and complex robotic motion planning","10.1016/j.robot.2025.105167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014022990&doi=10.1016%2Fj.robot.2025.105167&partnerID=40&md5=d0fa774cc323c437026be6ee05fa990d","Motion planning is the soul of robot decision making. Classical planning algorithms like graph search and reaction-based algorithms face challenges in cases of dense and dynamic obstacles. Deep learning algorithms generate suboptimal one-step predictions that cause many collisions. Reinforcement learning algorithms generate optimal or near-optimal time-sequential predictions. However, they suffer from slow convergence, suboptimal converged results, and unstable training. This paper introduces a hybrid algorithm for robotic motion planning: long short-term memory (LSTM) and skip connection for attention-based discrete soft actor critic (LSA-DSAC). First, graph network (relational graph) and attention network (attention weight) interpret the environmental state for the learning of the discrete soft actor critic algorithm. The expressive power of attention network outperforms that of graph in our task by difference analysis of these two representation methods. However, attention based DSAC faces the problem of unstable training (vanishing gradient). Second, the skip connection method is integrated to attention based DSAC to mitigate unstable training and improve convergence speed. Third, LSTM is taken to replace the sum operator of attention weigh and eliminate unstable training by slightly sacrificing convergence speed at early-stage training. Experiments show that LSA-DSAC outperforms the state-of-the-art in training and most evaluations. Physical robots are also implemented and tested in the real world. © 2025 The Author(s)"
"Probabilistic Sampling Networks for Hybrid Structure Planning in Semi-Structured Environments","10.3390/s25206476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020256331&doi=10.3390%2Fs25206476&partnerID=40&md5=dba3ab9f3c30f16733b441f6c4f3316e","Highlights: What are the main findings? A hybrid structure planning method based on a Probabilistic Sampling Network (PSNet) and an Enhanced Artificial Potential Field (EAPF) is proposed to address high-dimensional robot motion planning in semi-structured environments. Experiments demonstrate that the proposed method outperforms MPNet and RRT-Connect in both 2-D point-mass robot and 6-DOF manipulator tasks, achieving higher success rates and more stable collision avoidance. What is the implication of the main finding? The proposed approach enhances the adaptability and robustness of industrial robots in intelligent manufacturing, maintaining efficient path planning in dynamic and complex scenarios. This study provides a new perspective for integrating learning-based methods with classical planning techniques, laying the foundation for future applications in autonomous robotic operations and human–robot collaboration. The advancement of adaptable industrial robots in intelligent manufacturing is hindered by the inefficiency of traditional motion planning methods in high-dimensional spaces. Therefore, a Dempster–Shafer evidence theory-based hybrid motion planner is proposed, in which a probabilistic sampling network (PSNet) and an enhanced artificial potential field (EAPF) cooperate with each other to improve the planning performance. The PSNet architecture comprises two modules: a motion planning module (MPM) and a fusion sampling module (FSM). The MPM utilizes sensor data alongside the robot’s current and target configurations to recursively generate diverse multimodal distributions of the next configuration. Based on the distribution information, the FSM was used as a decision-maker to ultimately generate globally connectable paths. Moreover, the FSM is equipped to correct collision path points caused by network inaccuracies through Gaussian resampling. Simultaneously, an augmented artificial potential field with a dynamic rotational field is deployed to repair local paths when worst-case collision scenarios occur. This collaborative strategy harmoniously unites the complementary strengths of both components, thereby enhancing the overall resilience and adaptability of the motion planning system. Experiments were conducted in various environments. The results demonstrate that the proposed method can quickly find directly connectable paths in diverse environments while reliably avoiding sudden obstacles. © 2025 by the authors."
"Interactive Environment-Aware Planning System and Dialogue for Social Robots in Early Childhood Education","10.3390/app152011107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020009834&doi=10.3390%2Fapp152011107&partnerID=40&md5=a2ccdcc8bbcf34791ee14d09caffa804","In this study, we propose an interactive environment-aware dialog and planning system for social robots in early childhood education, aimed at supporting the learning and social interaction of young children. The proposed architecture consists of three core modules. First, semantic simultaneous localization and mapping (SLAM) accurately perceives the environment by constructing a semantic scene representation that includes attributes such as position, size, color, purpose, and material of objects, as well as their positional relationships. Second, the automated planning system enables stable task execution even in changing environments through planning domain definition language (PDDL)-based planning and replanning capabilities. Third, the visual question answering module leverages scene graphs and SPARQL conversion of natural language queries to answer children’s questions and engage in context-based conversations. The experiment conducted in a real kindergarten classroom with children aged 6 to 7 years validated the accuracy of object recognition and attribute extraction for semantic SLAM, the task success rate of the automated planning system, and the natural language question answering performance of the visual question answering (VQA) module.The experimental results confirmed the proposed system’s potential to support natural social interaction with children and its applicability as an educational tool. © 2025 by the authors."
"Experience-Driven NeuroSymbolic System for Efficient Robotic Bolt Disassembly","10.3390/batteries11090332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017010249&doi=10.3390%2Fbatteries11090332&partnerID=40&md5=0a9b6756ba2d98e5e0f2370708237efa","With the rapid growth of electric vehicles, the efficient and safe recycling of high-energy battery packs, particularly the removal of structural bolts, has become a critical challenge. This study presents a NeuroSymbolic robotic system for battery disassembly, driven by autonomous learning capabilities. The system integrates deep perception modules, symbolic reasoning, and action primitives to achieve interpretable and efficient disassembly. To improve adaptability, we introduce an offline learning framework driven by a large language model (LLM), which analyzes historical disassembly trajectories and generates optimized action sequences via prompt-based reasoning. This enables the synthesis of new action primitives tailored to familiar scenarios. The system is validated on a real-world UR10e robotic platform across various battery configurations. Experimental results show a 17 s reduction in average disassembly time per bolt and a 154.4% improvement in overall efficiency compared with traditional approaches. These findings demonstrate that combining neural perception, symbolic reasoning, and LLM-guided learning significantly enhances robotic disassembly performance and offers strong potential for generalization in future battery recycling applications. © 2025 by the authors."
"Comparative Benchmark of Sampling-Based and DRL Motion Planning Methods for Industrial Robotic Arms","10.3390/s25175282","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015894601&doi=10.3390%2Fs25175282&partnerID=40&md5=87e58fb55e71fe252400e948e067aee8","This study presents a comprehensive comparison between classical sampling-based motion planners from the Open Motion Planning Library (OMPL) and a learning-based planner based on Soft Actor–Critic (SAC) for motion planning in industrial robotic arms. Using a UR3e robot equipped with an RG2 gripper, we constructed a large-scale dataset of over 100,000 collision-free trajectories generated with MoveIt-integrated OMPL planners. These trajectories were used to train a DRL agent via curriculum learning and expert demonstrations. Both approaches were evaluated on key metrics such as planning time, success rate, and trajectory smoothness. Results show that the DRL-based planner achieves higher success rates and significantly lower planning times, producing more compact and deterministic trajectories. Time-optimal parameterization using TOPPRA ensured the dynamic feasibility of all trajectories. While classical planners retain advantages in zero-shot adaptability and environmental generality, our findings highlight the potential of DRL for real-time and high-throughput motion planning in industrial contexts. This work provides practical insights into the trade-offs between traditional and learning-based planning paradigms, paving the way for hybrid architectures that combine their strengths. © 2025 by the authors."
"Vision-Based Navigation and Perception for Autonomous Robots: Sensors, SLAM, Control Strategies, and Cross-Domain Applications—A Review","10.3390/eng6070153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011609737&doi=10.3390%2Feng6070153&partnerID=40&md5=18d46b7a0b789c2b9200a4be5065e95f","Camera-centric perception has matured into a cornerstone of modern autonomy, from self-driving cars and factory cobots to underwater and planetary exploration. This review synthesizes more than a decade of progress in vision-based robotic navigation through an engineering lens, charting the full pipeline from sensing to deployment. We first examine the expanding sensor palette—monocular and multi-camera rigs, stereo and RGB-D devices, LiDAR–camera hybrids, event cameras, and infrared systems—highlighting the complementary operating envelopes and the rise of learning-based depth inference. The advances in visual localization and mapping are then analyzed, contrasting sparse and dense SLAM approaches, as well as monocular, stereo, and visual–inertial formulations. Additional topics include loop closure, semantic mapping, and LiDAR–visual–inertial fusion, which enables drift-free operation in dynamic environments. Building on these foundations, we review the navigation and control strategies, spanning classical planning, reinforcement and imitation learning, hybrid topological–metric memories, and emerging visual language guidance. Application case studies—autonomous driving, industrial manipulation, autonomous underwater vehicles, planetary rovers, aerial drones, and humanoids—demonstrate how tailored sensor suites and algorithms meet domain-specific constraints. Finally, the future research trajectories are distilled: generative AI for synthetic training data and scene completion; high-density 3D perception with solid-state LiDAR and neural implicit representations; event-based vision for ultra-fast control; and human-centric autonomy in next-generation robots. By providing a unified taxonomy, a comparative analysis, and engineering guidelines, this review aims to inform researchers and practitioners designing robust, scalable, vision-driven robotic systems. © 2025 by the authors."
"Explainable Robot Navigation","10.1609/aaai.v39i28.35208","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003905465&doi=10.1609%2Faaai.v39i28.35208&partnerID=40&md5=435501b37afaf0b1490ce12aaf62840e","As the use of autonomous mobile robots expands into dynamic and complex environments, the need for them to provide understandable explanations for their actions becomes crucial. This thesis addresses the challenge of developing explainability for robot navigation by leveraging a hybrid model that combines machine learning techniques with symbolic reasoning methods. Furthermore, the thesis explores the modeling of human explanation preferences and the impact of different explanation attributes on explanation recipients' understanding, satisfaction, and trust. The goal is to integrate different explanation aspects and approaches into a unified framework to support explainable navigation in robotics. © © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Large Language Models as Narrative Planning Search Guides","10.1109/TG.2024.3487416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208225574&doi=10.1109%2FTG.2024.3487416&partnerID=40&md5=30848083fcb72b866c9db105c6e63197","Symbolic planning algorithms and large language models have different strengths and weaknesses for story generation, suggesting hybrid models might leverage advantages from both. Others have proposed using a language model in combination with a partial order planning style algorithm to avoid the need for a hand-written symbolic domain of actions, or generating these domains from natural language input. This article offers a complementary approach. We propose to use a state space planning algorithm to plan coherent multiagent stories using hand-written symbolic domains, but with a language model acting as a guide to estimate, which events are worth exploring first. We present an initial evaluation of this approach on a set of benchmark narrative planning problems. © 2018 IEEE."
"CLIP-LLM: A Framework for Autonomous Plant Disease Management in Greenhouse","10.5220/0013674200003982","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022129202&doi=10.5220%2F0013674200003982&partnerID=40&md5=5a3e084ad20ed199ba3e6349e6b06cd6","Agricultural disease detection and intervention remain challenging due to complex crop health variations, dynamic environmental conditions, and labor-intensive fieldwork. We introduce an end-to-end, platform-agnostic robotic pipeline for autonomous disease detection and treatment systems, with a specific focus on cassava leaves as an example. The pipeline integrates a vision-language perception module based on a pretrained Contrastive Language-Image Pre-training (CLIP) model, fine-tuned on an augmented dataset of cassava leaf images for disease detection. High-level task planning is performed by a Generative Pre-trained Transformer 4 (GPT-4), which interprets perception outputs and generates symbolic action plans (e.g., navigate to target, perform treatment). The low-level control system is implemented in the PyBullet dynamic simulator. We evaluated a vision-language model (VLM) perception and a Large Language Model (LLM) based planning system (in a virtual environment with predefined 3D coordinates for plant and spray positions). The VLM achieved 83% classification accuracy in simulation and real-time tests with a static camera produced classification accuracies of 70% Cassava Brown Streak Disease (CBSD), 65% Cassava Mosaic Disease (CMD) and 52% Cassava Bacterial Blight (CBB), and under dynamic camera it achieve the accuracy of 65% (CBSD), 52% (CMD), and 32% (CBB). Currently, our low-level controller executes the LLM-generated plans with high precision (less than ±2 mm positioning error). These results demonstrate the viability of our platform-agnostic modular architecture for precision agriculture that supports closed-loop robustness and scalability. © 2025 by SCITEPRESS–Science and Technology Publications, Lda."
"SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches","10.24963/ijcai.2025/214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021818304&doi=10.24963%2Fijcai.2025%2F214&partnerID=40&md5=daf5b16fe70723b42a7d5883a0ff7df5","Hand-drawn sketches are a natural and efficient medium for capturing and conveying ideas. Despite significant advancements in controllable natural image generation, translating freehand sketches into structured, machine-readable diagrams remains a labor-intensive and predominantly manual task. The primary challenge stems from the inherent ambiguity of sketches, which lack the structural constraints and semantic precision required for automated diagram generation. To address this challenge, we introduce SketchAgent, a multiagent system designed to automate the transformation of hand-drawn sketches into structured diagrams. SketchAgent integrates sketch recognition, symbolic reasoning, and iterative validation to produce semantically coherent and structurally accurate diagrams, significantly reducing the need for manual effort. To evaluate the effectiveness of our approach, we propose the Sketch2Diagram Benchmark, a comprehensive dataset and evaluation framework encompassing eight diverse diagram categories, such as flowcharts, directed graphs, and model architectures. The dataset comprises over 6,000 high-quality examples with token-level annotations, standardized preprocessing, and rigorous quality control. By streamlining the diagram generation process, SketchAgent holds great promise for applications in design, education, and engineering, while offering a significant step toward bridging the gap between intuitive sketching and machine-readable diagram generation. © 2025 International Joint Conferences on Artificial Intelligence. All rights reserved."
"Agentic AI-Enhanced Virtual Reality for Adaptive Immersive Learning Environments","10.58496/MJCSC/2025/026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021348558&doi=10.58496%2FMJCSC%2F2025%2F026&partnerID=40&md5=12a15845529e6b03a305b6a8add33d0a","Immersive learning using Virtual Reality (VR) has gained prominence for delivering experiential, engaging education. However, most VR learning environments lack real-time adaptability, personalization, and cognitive responsiveness. This study presents an Agentic AI-enabled VR framework that autonomously adjusts pedagogical content, interaction style, and challenge level based on learner behavior, emotions, and performance feedback. The proposed system integrates a reinforcement learning-based agent with a virtual reality module to form an intelligent tutor capable of independent decision-making. A neuro-symbolic model processes multi-modal feedback (gesture, speech, gaze, performance) to determine context-aware pedagogical strategies. The system employs a self-evolving curriculum logic that adapts in real time. Experiments are conducted using Unity3D integrated with Python-based RL agents and simulated student models. Results demonstrate a 35.7% improvement in learner retention and a 42.1% reduction in cognitive overload compared to traditional static VR systems. The agent successfully personalized 92.3% of scenarios without human intervention. Emotional adaptivity and dynamic pacing showed increased engagement and reduced frustration metrics among diverse learners. Agentic VR represents a paradigm shift in intelligent education systems, enabling autonomous, emotionally-aware, and responsive learning environments. The proposed framework outperforms conventional VR platforms by offering real-time adaptive learning without pre-scripted logic. Integrating reinforcement learning, neuro-symbolic reasoning, and affective feedback into a single VR space results in a novel contribution to adaptive educational technology. The reported metrics were obtained from an administrator-controlled user study of 10 participants (aged 18-25 years), all of whom participated in three VR-based learning experiences as described in a controlled evaluative protocol. This study provides a foundation for future work on autonomous agents in educational met verses, special education, and lifelong learning systems. © 2025, Mesopotamian Academic Press. All rights reserved."
"DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021029408&partnerID=40&md5=f903520edb0232b02683262efb144faf","We introduce Dynamic Retrieval-Augmented Expert Networks (DRAE), a groundbreaking architecture that addresses the challenges of lifelong learning, catastrophic forgetting, and task adaptation by combining the dynamic routing capabilities of Mixture-of-Experts (MoE); leveraging the knowledge-enhancement power of Retrieval-Augmented Generation (RAG); incorporating a novel hierarchical reinforcement learning (RL) framework; and coordinating through ReflexNet-SchemaPlanner-HyperOptima (RSHO).DRAE dynamically routes expert models via a sparse MoE gating mechanism, enabling efficient resource allocation while leveraging external knowledge through parametric retrieval (P-RAG) to augment the learning process. We propose a new RL framework with ReflexNet for low-level task execution, SchemaPlanner for symbolic reasoning, and HyperOptima for long-term context modeling, ensuring continuous adaptation and memory retention. Experimental results show that DRAE significantly outperforms baseline approaches in long-term task retention and knowledge reuse, achieving an average task success rate of 82.5% across a set of dynamic robotic manipulation tasks, compared to 74.2% for traditional MoE models. Furthermore, DRAE maintains an extremely low forgetting rate, outperforming state-of-the-art methods in catastrophic forgetting mitigation. These results demonstrate the effectiveness of our approach in enabling flexible, scalable, and efficient lifelong learning for robotics. © 2025 Association for Computational Linguistics."
"Adaptive Neuro-Symbolic framework with dynamic contextual reasoning: A novel framework for semantic understanding","10.3934/mbe.2025112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020762964&doi=10.3934%2Fmbe.2025112&partnerID=40&md5=dfa4bbe15bff1e57b7dda41da6527dd2","Despite significant advances in image processing, achieving human-like semantic understanding and explainability remains a formidable challenge. Current deep learning models excel at feature extraction but lack the ability to reason about relationships, interpret context, or provide transparent decision-making. To address these limitations, we propose the adaptive neuro-symbolic framework with dynamic contextual reasoning (ANS-DCR), a novel architecture that seamlessly integrates neural networks with symbolic reasoning. ANS-DCR introduces four key innovations: 1) A contextual embedding layer (CEL) that dynamically converts neural features into structured symbolic embeddings tailored to the scene’s context; 2) hierarchical knowledge graphs (HKGs) that encode multi-level object relationships and update in real-time on the basis of neural feedback; 3) an adaptive reasoning engine (ARE) that performs scalable, context-aware logical reasoning; and 4) an explainable decision-making module (EDM) that generates human-readable explanations, including counterfactuals, enhancing interpretability. This framework bridges the gap between pattern recognition and logical reasoning, enabling deeper semantic understanding and dynamic adaptability. We demonstrate ANS-DCR’s efficacy in complex scenarios such as autonomous driving, where it accurately interprets traffic scenes, predicts behaviors, and provides clear explanations for decisions. Experimental results show superior performance in semantic segmentation, contextual reasoning, and explainability compared with state-of-the-art methods. By combining the strengths of neural and symbolic paradigms, ANS-DCR sets a new benchmark for intelligent, transparent, and scalable image processing systems, offering transformative potential for applications in robotics, healthcare, and beyond. The source code of the proposed ANS-DCR is at github.com/livingjesus/ANS-DCR. ©2025 the Author(s), licensee AIMS Press."
"Bayesian Inverse Physics for Neuro-Symbolic Robot Learning","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020240592&partnerID=40&md5=dd53c0e33ce98bf0cccebb000ae2511f","Real-world robotic applications, from autonomous exploration to assistive technologies, require adaptive, interpretable, and data-efficient learning paradigms. While deep learning architectures and foundation models have driven significant advances in diverse robotic applications, they remain limited in their ability to operate efficiently and reliably in unknown and dynamic environments. In this position paper, we critically assess these limitations and introduce a conceptual framework for combining data-driven learning with deliberate, structured reasoning. Specifically, we propose leveraging differentiable physics for efficient world modeling, Bayesian inference for uncertainty-aware decision-making, and meta-learning for rapid adaptation to new tasks. By embedding physical symbolic reasoning within neural models, robots could generalize beyond their training data, reason about novel situations, and continuously expand their knowledge. We argue that such hybrid neuro-symbolic architectures are essential for the next generation of autonomous systems, and to this end, we provide a research roadmap to guide and accelerate their development. © 2025 O. Arriaga, R. Adam, M. Laux, L. Gutzeit, M. Ragni, J. Peters & F. Kirchner."
"JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020239322&partnerID=40&md5=927910347e10f466df14a9df1c3b361a","Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. Tobenefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from6.1% to 15.8%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge. © 2025 K. Zheng, K. Zhou, J. Gu, Y. Fan, J. Wang, Z. Di, X. He & X.E. Wang."
"Knowledge Driven Robotics (KDR)","10.1109/CASE58245.2025.11164082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018325625&doi=10.1109%2FCASE58245.2025.11164082&partnerID=40&md5=b523a59c0cafafe2b0096c9ec795a425","This paper presents a software architecture that enables practical deployment of robotic systems in roles that require the execution of tasks not well-defined at design time. The paradigm employed is a maximization of reuse, generalization, and interoperability of logical structures and robot tasks which can readily be composited, supporting a robust and flexible autonomy stack. The Knowledge Driven Robotics (KDR) software architecture achieves this by offering a unique set of interactions between symbolic planning, Behavior Trees, and database information. New composite tasks can readily be composed of highly parameterized atomic tasks to manipulate known classes of task objects. Key contributions include the ability of the behavior layer to query, transform, and utilize data in a flexible manner and to act as data management systems during execution, as well as surrounding infrastructure enabling the treatment of data as a first-class citizen. © 2025 IEEE."
"Curiosity-Driven Imagination: Discovering Plan Operators and Learning Associated Policies for Open-World Adaptation","10.1109/ICRA55743.2025.11128650","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016696921&doi=10.1109%2FICRA55743.2025.11128650&partnerID=40&md5=b1bfe22741b2a89218a685640b770f71","Adapting quickly to dynamic, uncertain environments - often called 'open worlds' - remains a major challenge in robotics. Traditional Task and Motion Planning (TAMP) approaches struggle to cope with unforeseen changes, are data-inefficient when adapting, and do not leverage world models during learning. We address this issue with a hybrid planning and learning system that integrates two models: a low-level neural network-based model that learns stochastic transitions and drives exploration via an Intrinsic Curiosity Module (ICM), and a high-level symbolic planning model that captures abstract transitions using operators, enabling the agent to plan in an 'imaginary' space and generate reward machines. Our evaluation in a robotic manipulation domain with sequential novelty injections demonstrates that our approach converges faster and outperforms state-of-the-art hybrid methods. © 2025 IEEE."
"Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning","10.1109/ICRA55743.2025.11127692","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016649065&doi=10.1109%2FICRA55743.2025.11127692&partnerID=40&md5=a4f630aa8bfe6d246df81fb95320dcf1","Imitation learning is a popular method for teaching robots new behaviors. However, most existing methods focus on teaching short, isolated skills rather than long, multistep tasks. To bridge this gap, imitation learning algorithms must not only learn individual skills but also an abstract understanding of how to sequence these skills to perform extended tasks effectively. This paper addresses this challenge by proposing a neuro-symbolic imitation learning framework. Using task demonstrations, the system first learns a symbolic representation that abstracts the low-level state-action space. The learned representation decomposes a task into easier subtasks and allows the system to leverage symbolic planning to generate abstract plans. Subsequently, the system utilizes this task decomposition to learn a set of neural skills capable of refining abstract plans into actionable robot commands. Experimental results in three simulated robotic environments demonstrate that, compared to baselines, our neuro-symbolic approach increases data efficiency, improves generalization capabilities, and facilitates interpretability. © 2025 IEEE."
"Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-Level Goal Decomposition","10.1109/ICRA55743.2025.11127617","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016564924&doi=10.1109%2FICRA55743.2025.11127617&partnerID=40&md5=a39e1c5687c33a38a633866b53e50708","In robotic task planning, symbolic planners using rule-based representations like PDDL are effective but struggle with long-sequential tasks in complicated environments due to exponentially increasing search space. Meanwhile, LLM-based approaches, which are grounded in artificial neural networks, offer faster inference and commonsense reasoning but suffer from lower success rates. To address the limitations of the current symbolic (slow speed) or LLM-based approaches (low accuracy), we propose a novel neuro-symbolic task planner that decomposes complex tasks into subgoals using LLM and carries out task planning for each subgoal using either symbolic or MCTS-based LLM planners, depending on the subgoal complexity. This decomposition reduces planning time and improves success rates by narrowing the search space and enabling LLMs to focus on more manageable tasks. Our method significantly reduces planning time while maintaining high success rates across three task planning domains, as well as real-world and simulated robotics environments. More details are available at http://graphics.ewha.ac.kr/LLMTAMP/. © 2025 IEEE."
"Chameleon: Fast-Slow Neuro-Symbolic Lane Topology Extraction","10.1109/ICRA55743.2025.11127684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016533195&doi=10.1109%2FICRA55743.2025.11127684&partnerID=40&md5=57668dc86f3dc14d684fcb565454fdbd","Lane topology extraction involves detecting lanes and traffic elements and determining their relationships, a key perception task for mapless autonomous driving. This task requires complex reasoning, such as determining whether it is possible to turn left into a specific lane. To address this challenge, we introduce neuro-symbolic methods powered by vision-language foundation models (VLMs). Existing approaches have notable limitations: (1) Dense visual prompting with VLMs can achieve strong performance but is costly in terms of both financial resources and carbon footprint, making it impractical for robotics applications. (2) Neuro-symbolic reasoning methods for 3D scene understanding fail to integrate visual inputs when synthesizing programs, making them ineffective in handling complex corner cases. To this end, we propose a fast-slow neuro-symbolic lane topology extraction algorithm, named Chameleon, which alternates between a fast system that directly reasons over detected instances using synthesized programs and a slow system that utilizes a VLM with a chain-of-thought design to handle corner cases. Chameleon leverages the strengths of both approaches, providing an affordable solution while maintaining high performance. We evaluate the method on the OpenLane-V2 dataset, showing consistent improvements across various baseline detectors. Our code, data, and models are publicly available at https://github.com/XR-Lee/neural-symbolic. © 2025 IEEE."
"Structured Intelligence: Merging Neural and Symbolic AI","10.1109/ICOCT64433.2025.11118474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016311214&doi=10.1109%2FICOCT64433.2025.11118474&partnerID=40&md5=168678001178bbdec3f0d92bfdf7fcf3","In order to close the gap between statistical learning and explicit knowledge representation, a new interdisciplinary field called neuro-symbolic AI combines deep learning with symbolic reasoning. Symbolic reasoning offers structured decision-making and logical inference, while deep learning excels at pattern recognition and feature extraction. Key architectures, hybrid models, and differentiable symbolic reasoning techniques are all covered in this paper's thorough analysis of recent developments in neuro-symbolic AI. We investigate its uses in a number of fields, such as automated reasoning, natural language processing, robotics, and explainable AI. We also examine issues like scalability, interpretability, and knowledge transfer that arise when combining neural and symbolic approaches. We conclude by outlining possible avenues for future research, emphasizing the necessity of enhancing neuro-symbolic AI systems' generalization, resilience, and effectiveness. © 2025 IEEE."
"Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture","10.1109/ICAD65464.2025.11114062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016150697&doi=10.1109%2FICAD65464.2025.11114062&partnerID=40&md5=647ff6f9c1c2afc8963e6b92b51764ce","Vision-language-action (VLA) models hold promise as generalist robotics solutions by translating visual and linguistic inputs into robot actions, yet they lack reliability due to their black-box nature and sensitivity to environmental changes. In contrast, cognitive architectures (CA) excel in symbolic reasoning and state monitoring but are constrained by rigid predefined execution. This work bridges these approaches by probing OpenVLA's hidden layers to uncover symbolic representations of object properties, relations, and action states, enabling integration with a CA for enhanced interpretability and robustness. Through experiments on LIBERO-spatial pick-and-place tasks, we analyze the encoding of symbolic states across different layers of OpenVLA's Llama backbone. Our probing results show consistently high accuracies (>0.90) for both object and action states across most layers, though contrary to our hypotheses, we did not observe the expected pattern of object states being encoded earlier than action states. We demonstrate an integrated DIARCOpenVLA system that leverages these symbolic representations for real-time state monitoring, laying the foundation for more interpretable and reliable robotic manipulation. © 2025 IEEE."
"Supporting Human-Robot Collaboration and Safety with the Proposed Explainable Neuro-Symbolic Reasoning","10.1007/978-3-032-00642-4_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013968609&doi=10.1007%2F978-3-032-00642-4_15&partnerID=40&md5=3681083bea94a03e8ac649c4c0270cff","In this article, an overview of the innovative architecture of a hybrid AI neuro-symbolic architecture which uses high-resolution deep vision with probabilistic first-order logic for safety monitoring and anomaly detection in Human-Robot Collaboration environment is proposed. Firstly, the problem and the proposed solution and its application to human-robot collaboration scenarios is outlined. Then, the performance of the proposed method for anomaly detection and its conformity to the requirements defined by the end-users in realistic scenarios is discussed. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025."
"Synthesizing Evolving Symbolic Representations for Autonomous Systems","10.1007/s13748-025-00394-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013789361&doi=10.1007%2Fs13748-025-00394-9&partnerID=40&md5=3e8269271c82fe03264f8daf273ac800","This work presents a novel architecture for an open-ended learning system that integrates intrinsic motivation (IM) and classical planning to enable agents continuously learn and improve their knowledge over time in an unsupervised fashion. The main goal is to allow the agent to autonomously distill its experience into Probabilistic Planning Domain Definition Language (PPDDL) terms, thereby making causal relationships explicit and supporting automated planning. Starting with a virtually empty set of predefined tasks or goals, the agent harnesses intrinsic motivation to explore the environment autonomously, continuously using and enriching the high-level knowledge acquired through its experience in a virtuous cycle. Experimental evaluation in the Treasure Game domain demonstrates the effectiveness of the proposed approach: starting with only a small set of primitive actions, we show how an agent can autonomously build and refine a high-level representation of the environment. Planning-based strategies grounded in this representation significantly outperform uninformed exploration by reaching intermediate sub-goals more efficiently and substantially reducing the time required to achieve the final objective. © The Author(s) 2025."
"Imperative learning: A self-supervised neuro-symbolic learning framework for robot autonomy","10.1177/02783649251353181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012871130&doi=10.1177%2F02783649251353181&partnerID=40&md5=e1a8977177a522122941219bd689a759","Data-driven methods such as reinforcement and imitation learning have achieved remarkable success in robot autonomy. However, their data-centric nature still hinders them from generalizing well to ever-changing environments. Moreover, labeling data for robotic tasks is often impractical and expensive. To overcome these challenges, we introduce a new self-supervised neuro-symbolic (NeSy) computational framework, imperative learning (IL), for robot autonomy, leveraging the generalization abilities of symbolic reasoning. The framework of IL consists of three primary components: a neural module, a reasoning engine, and a memory system. We formulate IL as a special bilevel optimization (BLO), which enables reciprocal learning over the three modules. This overcomes the label-intensive obstacles associated with data-driven approaches and takes advantage of symbolic reasoning concerning logical reasoning, physical principles, geometric analysis, etc. We discuss several optimization techniques for IL and verify their effectiveness in five distinct robot autonomy tasks including path planning, rule induction, optimal control, visual odometry, and multi-robot routing. Through various experiments, we show that IL can significantly enhance robot autonomy capabilities and we anticipate that it will catalyze further research across diverse domains. © The Author(s) 2025"
"Hybrid intelligence systems for reliable automation: advancing knowledge work and autonomous operations with scalable AI architectures","10.3389/frobt.2025.1566623","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012031405&doi=10.3389%2Ffrobt.2025.1566623&partnerID=40&md5=29df8d6935b64977554b58fb38d8e4fa","Introduction: Mission-critical automation demands decision-making that is explainable, adaptive, and scalable—attributes elusive to purely symbolic or data-driven approaches. We introduce a hybrid intelligence (H-I) system that fuses symbolic reasoning with advanced machine learning via a hierarchical architecture, inspired by cognitive frameworks like Global Workspace Theory (Baars, A Cognitive Theory of Consciousness, 1988). Methods: This architecture operates across three levels to achieve autonomous, end-to-end workflows: Navigation: Using Vision Transformers, and graph-based neural networks, the system navigates file systems, databases, and software interfaces with precision. Discrete Actions: Multi-framework automated machine learning (AutoML) trains agents to execute discrete decisions, augmented by Transformers and Joint Embedding Predictive Architectures (JEPA) (Assran et al., 2023, 15619–15629) for complex time-series analysis, such as anomaly detection. Planning: Reinforcement learning, world model-based reinforcement learning, and model predictive control orchestrate adaptive workflows tailored to user requests or live system demands. Results: The system’s capabilities are demonstrated in two mission-critical applications: Space Domain Awareness, Satellite Behavior Detection: A graph-based JEPA paired with multi-agent reinforcement learning enables near real-time anomaly detection across 15,000 on-orbit objects, delivering a precision-recall score of 0.98. Autonomously Driven Simulation Setup: The system autonomously configures Computational Fluid Dynamics (CFD) setups, with an AutoML-driven optimizer enhancing the meshing step—boosting boundary layer capture propagation (BL-CP) from 8% to 98% and cutting geometry failure rates from 88% to 2% on novel aircraft geometries. Scalability is a cornerstone, with the distributed training pipeline achieving linear scaling across 2,000 compute nodes for AI model training, while secure model aggregation incurs less than 4% latency in cross-domain settings. Discussion: By blending symbolic precision with data-driven adaptability, this hybrid intelligence system offers a robust, transferable framework for automating complex knowledge work in domains like space operations and engineering simulations—and adjacent applications such as autonomous energy and industrial facility operations, paving the way for next-generation industrial AI systems. © © 2025 Grosvenor, Zemlyansky, Wahab, Bohachov, Dogan and Deighan."
"A Grid Cell-Inspired Structured Vector Algebra for Cognitive Maps","10.1109/NICE65350.2025.11065704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011415926&doi=10.1109%2FNICE65350.2025.11065704&partnerID=40&md5=8e75879314ff48049c258378078a6b8b","The entorhinal-hippocampal formation is the mammalian brain's navigation system, encoding both physical and abstract spaces via grid cells. This system is well-studied in neuroscience, and its efficiency and versatility make it attractive for applications in robotics and machine learning.While continuous attractor networks (CANs) successfully model entorhinal grid cells for encoding physical space, integrating both continuous spatial and abstract spatial computations into a unified framework remains challenging. Here, we attempt to bridge this gap by proposing a mechanistic model for versatile information processing in the entorhinal-hippocampal formation inspired by CANs and Vector Symbolic Architectures (VSAs), a neuro-symbolic computing framework. The novel grid-cell VSA (GC-VSA) model employs a spatially structured encoding scheme with 3D neuronal modules mimicking the discrete scales and orientations of grid cell modules, reproducing their characteristic hexagonal receptive fields.In experiments, the model demonstrates versatility in spatial and abstract tasks: (1) accurate path integration for tracking locations, (2) spatio-temporal representation for querying object locations and temporal relations, and (3) symbolic reasoning using family trees as a structured test case for hierarchical relationships. © 2025 IEEE."
"Generalised BDI Planning","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009819168&partnerID=40&md5=2e74a88653eade07bdfb994ca2a70bb4","Agent interpreters based on the Beliefs, Desires, and Intentions (BDI) model traditionally perform means-ends reasoning using plan libraries composed of reactive planning rules. However, the design of such rules often imposes a heavy knowledge engineering burden on a designer, and trades off flexibility for runtime efficiency. This use of planning rules originates from the limitations of planning technology at the time of the first BDI implementations. While these limitations have gradually been overcome by the integration of various types of planning into existing BDI theories, the corresponding interpreters remain fundamentally plan-library based. In this paper, we develop a novel BDI agent architecture driven by generalised planning as means-ends reasoning, in a radical departure from existing architectures. This architecture has two key properties. First, it more closely resembles the foundations of BDI logic and reasoning. Second, it offers substantial gains in efficiency in comparison with an architecture driven by classical planning. © 2025 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org)."
"The A-BDI Metamodel for Human-Level AI: Argumentation as Balancing, Dialogue and Inference","10.1007/978-981-96-7956-0_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008972426&doi=10.1007%2F978-981-96-7956-0_22&partnerID=40&md5=7e55eb0ad71dcd98e5704f3353fc9934","In this paper, we introduce A-BDI, the first metamodel for formal and computational argumentation. It contains three models, conceptualizing argumentation as balancing, argumentation as dialogue, and argumentation as inference respectively. Each model looks at argumentation from a different perspective, addressing its own concerns and using its own formal and computational methods. Whereas balancing is inspired by the scale metaphor and uses quantitative techniques typically found in theories in economics and neural computing, dialogue is developed in multiagent communication and interaction and uses chatbot and Large Language Models (LLMs) technology, and inference is derived from theoretical investigations in knowledge representation and reasoning and uses techniques from symbolic reasoning. By bringing together new and traditional Artificial Intelligence (AI) approaches, the A-BDI metamodel provides a formal and computational framework for human-level, neuro-symbolic, and hybrid AI. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025."
"Generative AI-Enhanced Neuro-Symbolic Quantum Architectures for Secure Communications and Networking","10.1109/MNET.2025.3579680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008830216&doi=10.1109%2FMNET.2025.3579680&partnerID=40&md5=26d5aec0fdca0b1ce0f951789094d523","The rapid convergence of generative AI (GAI), neuro-symbolic reasoning, and quantum computing has redefined secure communication and networking. Owing to the massive amounts of data, the need for extensive computing power, and the necessity to find and mitigate threats in real-time, traditional security measures cannot keep up with changing cyber threats as the communication infrastructure becomes more complicated. Therefore, this paper investigates the potential of a GAI-enhanced neuro-symbolic quantum framework for building security systems that are strong, scalable, and self-sufficient. This method improves threat detection, encryption, and real-time adversarial defense by combining symbolic reasoning for logic, deep learning for pattern recognition, and quantum intelligence to accelerate computations. This work has made progress in creating the next generation of secure networks that can automatically and adaptively make security decisions in real-time, providing robust protection in communication environments that change over time. © 1986-2012 IEEE."
"Towards Trustworthy and Explainable Socially Assistive Robots: A Cognitive Architecture for Dietary Guidance","10.1109/SIMPAR62925.2025.10979036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005026403&doi=10.1109%2FSIMPAR62925.2025.10979036&partnerID=40&md5=af677204be53856b9d3cbc914ffe9874","Socially Assistive Robots (SARs) are rising as promising tools for promoting healthy lifestyle habits. To achieve such a goal, it is necessary that they are able to perform trustworthy and legible behaviors. In this work, we propose a cognitive architecture that integrates multimodal perception, symbolic reasoning, memory-enhanced decision-making, and adaptive interaction strategies to create an explainable and engaging dietary assistant. The key idea is to provide the robot with the capability to iteratively interact with a user and adapt the dietary plan based on their current state, preferences, and food restrictions, while conveying explicitly the inner decision and thought process. To achieve this, we employ a graph-enhanced Large Language Model (LLM), which queries contextual, semantic, and episodic acquired knowledge to generate personalized meal recommendations. These must subsequently be refined through a verification process that enforces constraints such as caloric limits and ingredient intolerances, ensuring dietary adherence. To have a transparent decision-verification process, the robot has to progressively verbalize the reasoning process while providing justifications for the recommendations to also enhance the user's trust. Non-verbal context-relevant movements are also generated to allow the robot to express empathy. We expect our framework to increase user trust, engagement, and adherence to healthy behaviors, allowing SARs to function as credible and effective health assistants. © 2025 IEEE."
"Bridging the Gap: The Rise of Neurosymbolic Artificial Intelligence in Advanced Computing","10.1109/MITP.2025.3532388","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003759861&doi=10.1109%2FMITP.2025.3532388&partnerID=40&md5=a92a55fd79ae83062f6bfb553657a138","Neurosymbolic artificial intelligence (AI) has emerged as a transformative approach, integrating the reasoning capabilities of symbolic AI with the data-driven nature of neural networks. This article discusses the evolution of AI, identifying the distinct advantages and limitations of symbolic and deep learning methodologies. Neurosymbolic AI aims to leverage the strengths of both approaches to enhance decision-making transparency and efficiency, which are crucial in high-stakes domains like health care and law. We explore techniques like embedding symbolic reasoning within neural architectures and utilizing neural networks for feature extraction. These strategies enable the application of neurosymbolic AI across various sectors, including health-care diagnostics, automated financial advising, and robotics. Case studies demonstrate its potential in improving diagnosis accuracy, ensuring regulatory compliance in finance, and enabling adaptive responses in robotics. The article concludes by highlighting emerging trends that are aimed at refining the interaction between neural and symbolic components, fostering more robust and versatile AI applications. © 2025 IEEE."
"Neuro-Symbolic Reasoning for Multimodal Referring Expression Comprehension in HMI Systems","10.1007/s00354-024-00243-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185111766&doi=10.1007%2Fs00354-024-00243-8&partnerID=40&md5=1f8e483d7e9e130890beef5f1c5645f6","Conventional Human–Machine Interaction (HMI) interfaces have predominantly relied on GUI and voice commands. However, natural human communication also consists of non-verbal communication, including hand gestures like pointing. Thus, recent works in HMI systems have tried to incorporate pointing gestures as an input, making significant progress in recognizing and integrating them with voice commands. However, existing approaches often treat these input modalities independently, limiting their capacity to handle complex multimodal instructions requiring intricate reasoning of language and gestures. On the other hand, multimodal tasks requiring complex reasoning are being challenged in the language and vision domain, but these typically do not include gestures like pointing. To bridge this gap, we explore one of the challenging multimodal tasks, called Referring Expression Comprehension (REC), within multimodal HMI systems incorporating pointing gestures. We present a virtual setup in which a robot shares an environment with a user and is tasked with identifying objects based on the user’s language and gestural instructions. Furthermore, to address this challenge, we propose a hybrid neuro-symbolic model combining deep learning’s versatility with symbolic reasoning’s interpretability. Our contributions include a challenging multimodal REC dataset for HMI systems, an interpretable neuro-symbolic model, and an assessment of its ability to generalize the reasoning to unseen environments, complemented by an in-depth qualitative analysis of the model’s inner workings. © The Author(s) 2024."
"A Practical Operational Semantics for Classical Planning in BDI Agents","10.3233/FAIA240636","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213365943&doi=10.3233%2FFAIA240636&partnerID=40&md5=5fe9eb3371589453fc24def49712b853","Implementations of the Belief-Desire-Intention (BDI) architecture have a long tradition in the development of autonomous agent systems. However, most practical implementations of the BDI framework rely on a pre-defined plan library for decision-making, which places a significant burden on programmers, and still yields systems that may be brittle, struggling to achieve their goals in dynamic environments. This paper overcomes this limitation by introducing an operational semantics for BDI systems that rely on Classical Planning at run time to both cope with failures that were unforeseeable and synthesise new plans that were unspecified at design time. This semantics places particular emphasis on the interaction of the reasoning cycle and an underlying planning algorithm. We empirically demonstrate the practical feasibility and generality of such an approach in an implementation of this semantics within two popular BDI platforms together with in-depth computational evaluation. © 2024 The Authors."
"Planning and learning to perceive in partially unknown environments","10.3233/IA-240036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214354489&doi=10.3233%2FIA-240036&partnerID=40&md5=f42c95e2216c3a5a94b2a967b49462e9","For applying symbolic planning, an agent acting in an environment needs to know its symbolic state, and an abstract model of the environment dynamics. However, in the real world, an agent has low-level perceptions of the environment (e.g. its position given by a GPS sensor), rather than symbolic observations representing its current state. Furthermore, in many real-world scenarios, it is not feasible to provide an agent with a complete and correct model of the environment, e.g., when the environment is (partially) unknown a priori. Therefore, agents need to dynamically learn/adapt/extend their perceptual abilities online, in an autonomous way, by exploring and interacting with the environment where they operate. In this paper, we provide a general architecture of a planning, learning, and acting agent. Moreover, we propose solutions to the problems of automatically training a neural network to recognize object properties, learning the situations where such properties are better perceivable, and planning to get into such situations. We experimentally show the effectiveness of our approach in simulated and complex environments, and we empirically demonstrate the feasibility of our approach in a real-world scenario that involves noisy perceptions and noisy actions on a real robot. © 2024 - IOS Press. All rights reserved."
"Planning with mental models – Balancing explanations and explicability","10.1016/j.artint.2024.104181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199368903&doi=10.1016%2Fj.artint.2024.104181&partnerID=40&md5=912d1d6d8274c1a745eecd96ac646e26","Human-aware planning involves generating plans that are explicable, i.e. conform to user expectations, as well as providing explanations when such plans cannot be found. In this paper, we bring these two concepts together and show how an agent can achieve a trade-off between these two competing characteristics of a plan. To achieve this, we conceive a first-of-its-kind planner MEGA that can reason about the possibility of explaining a plan in the plan generation process itself. We will also explore how solutions to such problems can be expressed as “self-explaining plans” – and show how this representation allows us to leverage classical planning compilations of epistemic planning to reason about this trade-off at plan generation time without having to incur the computational burden of having to search in the space of differences between the agent model and the mental model of the human in the loop in order to come up with the optimal trade-off. We will illustrate these concepts in two well-known planning domains, as well as with a robot in a typical search and reconnaissance task. Human factor studies in the latter highlight the usefulness of the proposed approach. © 2024"
"Optimization-based motion planning for autonomous agricultural vehicles turning in constrained headlands","10.1002/rob.22374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195568592&doi=10.1002%2Frob.22374&partnerID=40&md5=b302e71e2575ad1dc132677b6f904147","Headland maneuvering is a crucial part of the field operations performed by autonomous agricultural vehicles (AAVs). While motion planning for headland turning in open fields has been extensively studied and integrated into commercial autoguidance systems, the existing methods primarily address scenarios with ample headland space and thus may not work in more constrained headland geometries. Commercial orchards often contain narrow and irregularly shaped headlands, which may include static obstacles, rendering the task of planning a smooth and collision-free turning trajectory difficult. To address this challenge, we propose an optimization-based motion planning algorithm for headland turning under geometrical constraints imposed by headland geometry and obstacles. Our method models the headland and the AAV using convex polytopes as geometric primitives, and calculates optimal and collision-free turning trajectories in two stages. In the first stage, a coarse path is generated using either a classical pattern-based turning method or a directional graph-guided hybrid A* algorithm, depending on the complexity of the headland geometry. The second stage refines this coarse path by feeding it into a numerical optimizer, which considers the vehicle's kinematic, control, and collision-avoidance constraints to produce a feasible and smooth trajectory. We demonstrate the effectiveness of our algorithm by comparing it to the classical pattern-based method in various types of headlands. The results show that our optimization-based planner outperforms the classical planner in generating collision-free turning trajectories inside constrained headland spaces. Additionally, the trajectories generated by our planner respect the kinematic and control limits of the vehicle and, hence, are easier for a path-tracking controller to follow. In conclusion, our proposed approach successfully addresses complex motion planning problems in constrained headlands, making it a valuable contribution to the autonomous operation of AAVs, particularly in real-world orchard environments. © 2024 Wiley Periodicals LLC."
"Quality Diversity for Robot Learning: Limitations and Future Directions","10.1145/3638530.3654431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201964775&doi=10.1145%2F3638530.3654431&partnerID=40&md5=ddbd6bcbb0dfe0aa7e19ce6b172ea967","Quality Diversity (QD) has shown great success in discovering high-performing, diverse policies for robot skill learning. While current benchmarks have led to the development of powerful QD methods, we argue that new paradigms must be developed to facilitate open-ended search and generalizability. In particular, many methods focus on learning diverse agents that each move to a different xy position in MAP-Elites-style bounded archives. Here, we show that such tasks can be accomplished with a single, goal-conditioned policy paired with a classical planner, achieving O(1) space complexity w.r.t. the number of policies and generalization to task variants. We hypothesize that this approach is successful because it extracts task-invariant structural knowledge by modeling a relational graph between adjacent cells in the archive. We motivate this view with emerging evidence from computational neuroscience and explore connections between QD and models of cognitive maps in human and other animal brains. We conclude with a discussion exploring the relationships between QD and cognitive maps, and propose future research directions inspired by cognitive maps towards future generalizable algorithms capable of truly open-ended search. © 2024 Copyright held by the owner/author(s)."
"On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)","10.1609/icaps.v34i1.31503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192278898&doi=10.1609%2Ficaps.v34i1.31503&partnerID=40&md5=cd18cbe3fdbe68e81adc07f7f40d3150","Automated Planning and Scheduling is among the growing areas in Artificial Intelligence (AI) where mention of LLMs has gained popularity. Based on a comprehensive review of 126 papers, this paper investigates eight categories based on the unique applications of LLMs in addressing various aspects of planning problems: language translation, plan generation, model construction, multi-agent planning, interactive planning, heuristics optimization, tool integration, and brain-inspired planning. For each category, we articulate the issues considered and existing gaps. A critical insight resulting from our review is that the true potential of LLMs unfolds when they are integrated with traditional symbolic planners, pointing towards a promising neuro-symbolic approach. This approach effectively combines the generative aspects of LLMs with the precision of classical planning methods. By synthesizing insights from existing literature, we underline the potential of this integration to address complex planning challenges. We aim to keep the categorization of papers updated on https://ai4society.github.io/LLM-Planning-Viz/, a collaborative resource that allows researchers to contribute and add new literature to the categorization. © © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"GOALNET: Interleaving Neural Goal Predicate Inference with Classical Planning for Generalization in Robot Instruction Following","10.1609/aaai.v38i18.29990","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189560910&doi=10.1609%2Faaai.v38i18.29990&partnerID=40&md5=05a921018e9b1fb1f839b98461e0996f","Our goal is to enable a robot to learn how to sequence its actions to perform high-level tasks specified as natural language instructions, given successful demonstrations from a human partner. Our novel neuro-symbolic solution GOALNET builds an iterative two-step approach that interleaves (i) inferring next subgoal predicate implied by the language instruction, for a given world state, and (ii) synthesizing a feasible subgoal-reaching plan from that state. The agent executes the plan, and the two steps are repeated. GOALNET combines (i) learning, where dense representations are acquired for language instruction and the world state via a neural network prediction model, enabling generalization to novel settings and (ii) planning, where the cause-effect modeling by a classical planner eschews irrelevant predicates, facilitating multistage decision making in large domains. GOALNET obtains 78% improvement in the goal reaching rate in comparison to several state-of-the-art approaches on benchmark data with multi-stage instructions. Further, GOALNET can generalize to novel instructions for scenes with unseen objects. Source code available at https://github.com/reail-iitd/goalnet. © © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Teaching Robots with Show and Tell: Using Foundation Models to Synthesize Robot Policies from Language and Visual Demonstrations","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000719676&partnerID=40&md5=a092c5cd6eb8b7e0442e5b364505f1e7","We introduce a modular, neuro-symbolic framework for teaching robots new skills through language and visual demonstration. Our approach, SHOWTELL, composes a mixture of foundation models to synthesize robot manipulation programs that are easy to interpret and generalize across a wide range of tasks and environments. SHOWTELL is designed to handle complex demonstrations involving high level logic such as loops and conditionals while being intuitive and natural for end-users. We validate this approach through a series of real-world robot experiments, showing that SHOWTELL out-performs a state-of-the-art baseline based on GPT4-V, on a variety of tasks, and that it is able to generalize to unseen environments and within category objects. Supplementary materials and videos are available on our webpage: https://robo-showtell.github.io. © 2024 Proceedings of Machine Learning Research."
"Marching Forward: Redefining Human-Machine Interactions in Conversational AI Through Hybrid Intelligence, Blockchain Security, and Autonomous Agents","10.1109/ICCES63552.2024.10859659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218414183&doi=10.1109%2FICCES63552.2024.10859659&partnerID=40&md5=5a15f4edabb5f7dd772f27b4f2c67cd2","Conversational AI has emerged as an essential instrument in enhancing human-computer interaction, with applications spanning customer service to personal assistants. This paper offers a comprehensive examination of recent developments in conversational AI, including novel techniques in natural language generation (NLG), dialogue systems, and dynamic response optimization. We analyze advancements including transformer-based architectures, reinforcement learning for dialogue generation, and retrieval-augmented models, emphasizing their roles in enhancing the quality and contextual accuracy of conversations. Additionally, we examine the feasibility of blockchain technology as the foundation for decentralized AI, promoting secure, transparent, and distributed frameworks for AI applications, hence improving privacy and control in data exchanges. Hybrid methodologies that combine symbolic reasoning with deep learning are analyzed, highlighting their effectiveness in addressing complex dialogues. These advancements, supported by empirical research and benchmark performance, signify the evolution towards more personalized, intelligent, and decentralized conversational systems. © 2024 IEEE."
"A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM","10.1109/IROS58592.2024.10802379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216463398&doi=10.1109%2FIROS58592.2024.10802379&partnerID=40&md5=bddf795b591f39fa3fcbd7805b1910c8","Autonomous robots, autonomous vehicles, and humans wearing mixed-reality headsets require accurate and reliable tracking services for safety-critical applications in dynamically changing real-world environments. However, the existing tracking approaches, such as Simultaneous Localization and Mapping (SLAM), do not adapt well to environmental changes and boundary conditions despite extensive manual tuning. On the other hand, while deep learning-based approaches can better adapt to environmental changes, they typically demand substantial data for training and often lack flexibility in adapting to new domains. To solve this problem, we propose leveraging the neurosymbolic program synthesis approach to construct adaptable SLAM pipelines that integrate the domain knowledge from traditional SLAM approaches while leveraging data to learn complex relationships. While the approach can synthesize end-to-end SLAM pipelines, we focus on synthesizing the feature extraction module. We first devise a domain-specific language (DSL) that can encapsulate domain knowledge on the essential attributes for feature extraction and the real-world performance of various feature extractors. Our neurosymbolic architecture then undertakes adaptive feature extraction, optimizing parameters via learning while employing symbolic reasoning to select the most suitable feature extractor. Our evaluations demonstrate that our approach, neurosymbolic Feature EXtraction (nFEX), yields higher-quality features. It also reduces the pose error observed for the state-of-the-art baseline feature extractors ORB and SIFT by up to 90% and up to 66%, respectively, thereby enhancing the system's efficiency and adaptability to novel environments. © 2024 IEEE."
"A Framework for Neurosymbolic Goal-Conditioned Continual Learning in Open World Environments","10.1109/IROS58592.2024.10801627","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216460400&doi=10.1109%2FIROS58592.2024.10801627&partnerID=40&md5=72e97de0994c0ae4a3d5603aaf50e32d","In dynamic open-world environments, agents continually face new challenges due to sudden and unpredictable novelties, hindering Task and Motion Planning (TAMP) in autonomous systems. We introduce a novel TAMP architecture that integrates symbolic planning with reinforcement learning to enable autonomous adaptation in such environments, operating without human guidance. Our approach employs symbolic goal representation within a goal-oriented learning framework, coupled with planner-guided goal identification, effectively managing abrupt changes where traditional reinforcement learning, re-planning, and hybrid methods fall short. Through sequential novelty injections in our experiments, we assess our method's adaptability to continual learning scenarios. Extensive simulations conducted in a robotics domain corroborate the superiority of our approach, demonstrating faster convergence to higher performance compared to traditional methods. The success of our framework in navigating diverse novelty scenarios within a continuous domain underscores its potential for critical real-world applications. © 2024 IEEE."
"Learning to Ground Existentially Quantified Goals","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214661463&partnerID=40&md5=2d26a9a8b770da455c3751775f47ce8d","Goal instructions for autonomous AI agents cannot assume that objects have unique names. Instead, objects in goals must be referred to by providing suitable descriptions. However, this raises problems in both classical planning and generalized planning. The standard approach to handling existentially quantified goals in classical planning involves compiling them into a DNF formula that encodes all possible variable bindings and adding dummy actions to map each DNF term into the new, dummy goal. This preprocessing is exponential in the number of variables. In generalized planning, the problem is different: even if general policies can deal with any initial situation and goal, executing a general policy requires the goal to be grounded to define a value for the policy features. The problem of grounding goals, namely finding the objects to bind the goal variables, is subtle: it is a generalization of classical planning, which is a special case when there are no goal variables to bind, and constraint reasoning, which is a special case when there are no actions. In this work, we address the goal grounding problem with a novel supervised learning approach. A GNN architecture, trained to predict the cost of partially quantified goals over small domain instances is tested on larger instances involving more objects and different quantified goals. The proposed architecture is evaluated experimentally over several planning domains where generalization is tested along several dimensions including the number of goal variables and objects that can bind such variables. The scope of the approach is also discussed in light of the known relationship between GNNs and C2 logics. © 2024 Proceedings of the International Conference on Knowledge Representation and Reasoning. All rights reserved."
"Communication with Individuals with Disabilities and the Role of LLMs: Progress Report","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214206565&partnerID=40&md5=af8780337251e4d6057ca01d8e82bd36","This paper is part of a project aimed at leveraging neuro-symbolic architectures to achieve a sophisticated interaction between humanoid robots and individuals with cognitive impairments. In our architecture, a symbolic reasoning module based on Answer Set Programming creates a sequence of appropriate activities and monitors their execution in real-time. Large Language Models (LLMs) are used to enhance the user experience in various ways. Among those, in this paper we investigate methods for using LLMs to rewording text produced by the reasoning component, while keeping length and vocabulary level consistent with the original text. The study explores the effectiveness of metrics such as length and frequency of use, in comparing the vocabulary level of the input with that of the output. We present a comparative analysis of free or moderately priced LLMs, such as GPT-3.5, Google Gemini Pro, and Claude 3 Opus. A continuous validation process is also introduced, utilizing a critic that evaluates the appropriateness of the generated output at run-time. Although preliminary, the findings appear to indicate that while LLMs can often produce outputs with a vocabulary level comparable to the inputs, there are areas needing improvement, particularly in handling specific domain knowledge or less common phrases. This research contributes to the exploration of novel neuro-symbolic architectures and to the practical application of LLMs in contexts where controlled language use is essential for effective communication. © 2022 Copyright for this paper by its authors."
"A Hybrid Cognitive Architecture to Generate, Control, Plan, and Monitor Behaviors for Interactive Autonomous Robots","10.1007/s12369-024-01192-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211892521&doi=10.1007%2Fs12369-024-01192-4&partnerID=40&md5=0463b8677603355346dc9cac38834c7b","Interactive robots not only need to react in predefined or deterministic scenarios but also learn and adapt in real-time, mirroring cognitive flexibility akin to human intelligence. Achieving this autonomy entails developing cognitive architectures that integrate reactive, deliberative and emergent capabilities. Thus, this paper presents MERLIN2, a hybrid cognitive architecture to generate, control, plan, and monitor behaviors in autonomous robots. This architecture combines reactive, deliberative, and emergent components, aiming to enhance adaptability in dynamic environments and make intelligent real-time decisions, thereby improving autonomy and performance. MERLIN2 comprises a deliberative system, based on a knowledge base and a symbolic planner; and a behavioral system composed of reactive components and several emergent components. It addresses core cognitive aspects like action selection, perception, memory, learning, reasoning, and explainability. MERLIN2 is evaluated in a simulated world and in the real world Carry My Luggage task from the RoboCup@Home. Therefore, the experimentation presented in this article showcases the architecture as a valid solution for autonomous robots. © The Author(s) 2024."
"Cognitive Programming Interface : from Task Level Programming to Coherent Task Level Programming","10.1109/CASE59546.2024.10711577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208236644&doi=10.1109%2FCASE59546.2024.10711577&partnerID=40&md5=d7a6876d69de65e09b6342334387b43c","One of the main barriers to the uptake of robotics in industry is the cost of integration. For small and medium-sized enterprises with small production runs and frequent changes to their robotic cells, the cost of integration and programming is too high an investment. To tackle this problem, a number of techniques exist in the literature to perform Task-Level-Programming.However most provide a set of tasks without defining, or in a very limited way, their context of use and their effects on the robot's environment. As a result, users need to master a wide range of skills and the context in which they can be used.In this paper, we propose a different approach considering object's interfaces level instead of object level, to allow the user to perform Coherent Task Level Programming where available skills/tasks are context dependent. We describe a Cognitive Programming Interface based on a multimodal planner relying on semantic reasoning that allows two programming modalities: programming by selecting a feasible skill according to the context, or programming by demonstration. These two methods are complementary and can be used in the same programming sequence if required. They rely on an ontological skill definition that replaces the classical Planning Domain Definition Language approach. Consequently, we propose a new method to recognize demonstrated skills from demonstration. © 2024 IEEE."
"Adapting to the 'Open World': The Utility of Hybrid Hierarchical Reinforcement Learning and Symbolic Planning","10.1109/ICRA57147.2024.10611594","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202438527&doi=10.1109%2FICRA57147.2024.10611594&partnerID=40&md5=adf86ebffca4e6dde4c642cb76e46831","Open-world robotic tasks such as autonomous driving pose significant challenges to robot control due to unknown and unpredictable events that disrupt task performance. Neural network-based reinforcement learning (RL) techniques (like DQN, PPO, SAC, etc.) struggle to adapt in large domains and suffer from catastrophic forgetting. Hybrid planning and RL approaches have shown some promise in handling environmental changes but lack efficiency in accommodation speed. To address this limitation, we propose an enhanced hybrid system with a nested hierarchical action abstraction that can utilize previously acquired skills to effectively tackle unexpected novelties. We show that it can adapt faster and generalize better compared to state-of-the-art RL and hybrid approaches, significantly improving robustness when multiple environmental changes occur at the same time. © 2024 IEEE."
"Hybrid Personal Medical Digital Assistant Agents","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200151936&partnerID=40&md5=6ba8f91af0e2c01ec0e77a0e161bfaa0","Autonomous intelligent systems are beginning to impact clinical practice as personal medical assistant agents, by leveraging experts’ knowledge when needed and exploiting the vast amount of patient data available to clinicians. However, these approaches are seldom integrated. In this paper, we propose an integrated hybrid agent architecture that combines symbolic reasoning with sub-symbolic, data-driven models. Using the PIMA dataset, we demonstrate that this hybrid approach enhances the performance of both approaches when used alone. Specifically, we show that integrating a logical agent, which uses predefined expert knowledge plans, with rules obtained by symbolic knowledge extraction from machine learning models trained on historical data, improves system reliability and clinical decision-making, while reducing misclassified instances. © 2022 Copyright for this paper by its authors."
"Modular, Hierarchical Machine Learning for Sequential Goal Completion","10.1117/12.3013861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196479155&doi=10.1117%2F12.3013861&partnerID=40&md5=a42fc91c3a33257d09f67f345c58ae70","Given a maze populated with different objects, one may task a robot with a sequential goal completion task, e.g. 1) pick up a key then 2) unlock the door then 3) unlock the treasure chest. A typical machine learning (ML) solution would involve a monolithically trained artificial neural network (ANN). However, if the sequence of goals or the goals themselves change, then the ANN must be significantly (or, at worst, completely) retrained. Instead of a monolithic ANN, a modular ML component would be 1) independently optimizable (task-agnostic) and 2) arbitrarily reconfigurable with other ML modules. This work describes a modular, hierarchical ML framework by integrating two emerging ML techniques: 1) cognitive map learners (CML) and 2) hyperdimensional computing (HDC). A CML is a collection of three single layer ANNs (matrices) collaboratively trained to learn the topology of an abstract graph. Here, two CMLs were constructed, one describing locations on in 2D physical space and the other the relative distribution of objects found in this space. Each CML node states was encoded as a high-dimensional vector to utilize HDC, an ML algebra, for symbolic reasoning over these high-dimensional “symbol” vectors. In this way, each sub-goal above was described by algebraic equations of CML node states. Multiple, independently trained CMLs were subsequently assembled together to navigate a maze to solve a sequential goal task. Critically, changes to these goals required only localized changes in the CML-HDC architecture, as opposed to a global ANN retraining scheme. This framework therefore enabled a more traditional engineering approach to ML, akin to digital logic design. © 2024 SPIE."
"Fast and Slow Goal Recognition","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196390258&partnerID=40&md5=fbc2e0aeedb3dd8d763fef0c5c8d92aa","Goal recognition is a crucial aspect of understanding the intentions and objectives of agents by observing some of their actions. The most prominent approaches to goal recognition can be divided into two main categories: (1) trustworthy systems, which exploit automated reasoning for computing plans compatible with the observed actions, and (2) swifter systems, which try to quickly infer goals, often overlooking complex cognitive processes, and have no formal guarantees of their results. This paper introduces a novel approach inspired by the dual process theory, which integrates these two techniques. A dual-process model is proposed, leveraging fast, experience-based recognition for immediate goal identification, and slow, deliberate analysis for deeper understanding. Machine learning techniques and classical planning techniques are employed to obtain this dual-process system. Experimental evaluations demonstrate the effectiveness of the approach, reducing the amount of resources required to compute a solution (e.g., time to find a goal), while at the same time enhancing accuracy and robustness, especially in more complex scenarios. © 2024 International Foundation for Autonomous Agents and Multiagent Systems."
"Towards building Autonomous AI Agents and Robots for Open World Environments","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196357353&partnerID=40&md5=2ef236cae10407b09e57a4713a2326fb","The shift of AI agents from controlled laboratory environments to real-world applications, such as autonomous vehicles and service robots, demands robust algorithms for navigating the intricacies of open-world scenarios. While traditional AI agents show proficiency in predictable, closed-world settings, their performance often diminishes in the dynamic and unforeseen conditions of real-world environments. My dissertation focuses on developing methods, frameworks, and domains that push the boundaries of open-world problem-solving in AI agents and robots. The central thesis question explores how AI agents can rapidly learn and adapt in open-world settings while tackling long-horizon, complex tasks. My work proposes integrative frameworks that combine reinforcement learning with symbolic planning, enabling on-the-fly adaptation of agents. Furthermore, we also propose environments designed for developing and assessing agent architectures adept at handling novelty. These advancements in open-world learning are pivotal in enhancing adaptability, speed, and robustness in AI agents and robots, laying a critical foundation for the development of resilient autonomous systems. © 2024 International Foundation for Autonomous Agents and Multiagent Systems."
"A framework for neurosymbolic robot action planning using large language models","10.3389/fnbot.2024.1342786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196261608&doi=10.3389%2Ffnbot.2024.1342786&partnerID=40&md5=019f2ec4689b99bba60fce60663d98fe","Symbolic task planning is a widely used approach to enforce robot autonomy due to its ease of understanding and deployment in engineered robot architectures. However, techniques for symbolic task planning are difficult to scale in real-world, highly dynamic, human-robot collaboration scenarios because of the poor performance in planning domains where action effects may not be immediate, or when frequent re-planning is needed due to changed circumstances in the robot workspace. The validity of plans in the long term, plan length, and planning time could hinder the robot's efficiency and negatively affect the overall human-robot interaction's fluency. We present a framework, which we refer to as Teriyaki, specifically aimed at bridging the gap between symbolic task planning and machine learning approaches. The rationale is training Large Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner compatible with the Planning Domain Definition Language (PDDL), and then leveraging its generative capabilities to overcome a number of limitations inherent to symbolic task planners. Potential benefits include (i) a better scalability in so far as the planning domain complexity increases, since LLMs' response time linearly scales with the combined length of the input and the output, instead of super-linearly as in the case of symbolic task planners, and (ii) the ability to synthesize a plan action-by-action instead of end-to-end, and to make each action available for execution as soon as it is generated instead of waiting for the whole plan to be available, which in turn enables concurrent planning and execution. In the past year, significant efforts have been devoted by the research community to evaluate the overall cognitive capabilities of LLMs, with alternate successes. Instead, with Teriyaki we aim to providing an overall planning performance comparable to traditional planners in specific planning domains, while leveraging LLMs capabilities in other metrics, specifically those related to their short- and mid-term generative capabilities, which are used to build a look-ahead predictive planning model. Preliminary results in selected domains show that our method can: (i) solve 95.5% of problems in a test data set of 1,000 samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic planner; (iii) reduce average overall waiting times for a plan availability by up to 61.4%. © © 2024 Capitanelli and Mastrogiovanni."
"Semantic Knowledge-Based Mission Planning Method According to Robot Characteristics in Outdoor Environment","10.1007/978-3-031-44851-5_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192482423&doi=10.1007%2F978-3-031-44851-5_28&partnerID=40&md5=ae033b0e5ff77489ed9829beed6a8932","In the field of robots, many studies have been proposed to introduce advanced cognitive science into robot systems based on existing numerical information. These research directions allow robots to utilize their knowledge to simplify, understand, and efficiently plan their tasks. In this paper, we propose a semantic knowledge-based robot mission planning method in an irregular three-dimensional environment. We introduce an advanced robot mission planning method that defines the knowledge of robots and driving places based on ontology and utilizes the defined semantic knowledge. To this end, we define a robot domain based on Planning Domain Definition Language (PDDL) and solve it through a classical planner to plan the mission. Finally, we validate the suitability of the proposed method by conducting experiments with real-world robots within the campus environment and comparing them with general PDDL-based methods. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024."
"Real-world Application Facilitating Trustworthy Human-Robot Collaboration with Innovative Explainable Neuro-symbolic Reasoning","10.1109/ICDMW65004.2024.00053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001253961&doi=10.1109%2FICDMW65004.2024.00053&partnerID=40&md5=3f9e7b44736c90908bcdea9cf95f688d","With advancements in computer vision, robotics, and machine learning, human-robot collaboration is becoming increasingly prevalent in industrial and everyday contexts. The progressing and omnipresent need for automation of various aspects of our lives opens new challenges regarding safety and building trust between humans and autonomous robots. Although human-robot collaboration applications allow for increased productivity, safety is still a significant challenge when designing and deploying HRC systems.The hybrid AI approach developed and presented integrates deep learning algorithms and symbolic AI to efficiently detect potentially dangerous events. The goal is to increase the safety of the human operator and eventually to improve the trust in the robotic system. Moreover, we believe that such an innovative AI-based data analysis approach can lead to more fluent human-robot collaboration and improved productivity without sacrificing the safety of the human operators. Initial results demonstrate a high anomaly detection accuracy with a Balanced Accuracy of 99.9% and an F1-score of 99.8%, indicating our model's effectiveness in enhancing safe and efficient human-robot collaboration. © 2024 IEEE."
"LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000465921&partnerID=40&md5=1024eb6f6706e11284a733e758b782bb","Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks. However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions. Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities. To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents. LogiCity models diverse urban elements using semantic and spatial concepts, such as IsAmbulance(X) and IsClose(X, Y). These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios. Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning. To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors. Our extensive evaluation reveals the advantage of NeSy frameworks in abstract reasoning. Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data. With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI. All the code and data are open-sourced at our website. © 2024 Neural information processing systems foundation. All rights reserved."
"ECHO: A hierarchical combination of classical and multi-agent epistemic planning problems","10.1093/logcom/exad036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179892802&doi=10.1093%2Flogcom%2Fexad036&partnerID=40&md5=65e571062a983d4683d5a457654709f9","The continuous interest in Artificial Intelligence (AI) has brought, among other things, the development of several scenarios where multiple artificial entities interact with each other. As for all the other autonomous settings, these multi-agent systems require orchestration. This is, generally, achieved through techniques derived from the vast field of Automated Planning. Notably, arbitration in multi-agent domains is not only tasked with regulating how the agents act, but must also consider the interactions between the agents' information flows and must, therefore, reason on an epistemic level. This brings a substantial overhead that often diminishes the reasoning process's usability in real-world situations. To address this problem, we present ECHO, a hierarchical framework that embeds classical and multi-agent epistemic (epistemic, for brevity) planners in a single architecture. The idea is to combine (i) classical; and(ii) epistemic solvers to model efficiently the agents' interactions with the (i) 'physical world'; and(ii) information flows, respectively. In particular, the presented architecture starts by planning on the 'epistemic level', with a high level of abstraction, focusing only on the information flows. Then it refines the planning process, due to the classical planner, to fully characterize the interactions with the 'physical' world. To further optimize the solving process, we introduced the concept of macros in epistemic planning and enriched the 'classical' part of the domain with goal-networks. Finally, we evaluated our approach in an actual robotic environment showing that our architecture indeed reduces the overall computational time. © 2023 The Author(s). Published by Oxford University Press. All rights reserved."
"Beyond Traditional Neural Networks: Toward adding Reasoning and Learning Capabilities through Computational Logic Techniques","10.4204/EPTCS.385.51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173048555&doi=10.4204%2FEPTCS.385.51&partnerID=40&md5=389aae41043c51b744e45de6786545df","Deep Learning (DL) models have become popular for solving complex problems, but they have limitations such as the need for high-quality training data, lack of transparency, and robustness issues. Neuro-Symbolic AI has emerged as a promising approach combining the strengths of neural networks and symbolic reasoning. Symbolic Knowledge Injection (SKI) techniques are a popular method to incorporate symbolic knowledge into sub-symbolic systems. This work proposes solutions to improve the knowledge injection process and integrate elements of ML and logic into multi-agent systems (MAS). © Andrea Rafanelli."
"A memory system of a robot cognitive architecture and its implementation in ArmarX","10.1016/j.robot.2023.104415","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151796492&doi=10.1016%2Fj.robot.2023.104415&partnerID=40&md5=83622a56a7a2e6985e5b5a7015802a81","Cognitive agents such as humans and robots perceive their environment through an abundance of sensors producing streams of data that need to be processed to generate intelligent behavior. A key question of cognition-enabled and AI-driven robotics is how to organize and manage such data and knowledge efficiently in a cognitive robot control architecture. We argue, that memory is a central active component of such architectures that mediates between semantic and sensorimotor representations, orchestrates the flow of data streams and events between different processes and provides the components of a cognitive architecture with data-driven services for learning semantics from sensorimotor data, the parametrization of symbolic plans for execution and prediction of action effects. Based on related work, and the experience gained in developing our ARMAR humanoid robot systems, we identified conceptual and technical requirements of a memory system as central component of cognitive robot control architecture that facilitate the realization of high-level cognitive abilities such as explaining, reasoning, prospection, simulation and augmentation. Conceptually, a memory should be active, support multi-modal data representations, associate knowledge, be introspective, and have an inherently episodic structure. Technically, the memory should support a distributed design, be access-efficient and capable of long-term data storage. We introduce the memory system for our cognitive robot control architecture and its implementation in the robot software framework ArmarX. We evaluate the efficiency of the memory system with respect to transfer speeds, compression, reproduction and prediction capabilities. © 2023"
"Poster: How to Raise a Robot - Beyond Access Control Constraints in Assistive Humanoid Robots","10.1145/3589608.3595078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161251659&doi=10.1145%2F3589608.3595078&partnerID=40&md5=3bc1706c636327abfa8ad03eebd47e85","Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore incorporating privacy and security constraints (Activity-Centric Access Control and Deep Learning Based Access Control) with robot task planning approaches (classical symbolic planning and end-to-end learning-based planning). We report preliminary results on their respective trade-offs and conclude that a hybrid approach will most likely be the method of choice. © 2023 Owner/Author."
"A Digital Twin-Based Distributed Manufacturing Execution System for Industry 4.0 with AI-Powered On-The-Fly Replanning Capabilities","10.3390/su15076251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152785660&doi=10.3390%2Fsu15076251&partnerID=40&md5=218485de9da471dd0f7a05ce9775d6eb","Industry 4.0 smart production systems comprise industrial systems and subsystems that need to be integrated in such a way that they are able to support high modularity and reconfigurability of all system components. In today’s industrial production, manufacturing execution systems (MESs) and supervisory control and data acquisition (SCADA) systems are typically in charge of orchestrating and monitoring automated production processes. This article explicates an MES architecture that is capable of autonomously composing, verifying, interpreting, and executing production plans using digital twins and symbolic planning methods. To support more efficient production, the proposed solution assumes that the manufacturing process can be started with an initial production plan that may be relatively inefficient but quickly found by an AI. While executing this initial plan, the AI searches for more efficient alternatives and forwards better solutions to the proposed MES, which is able to seamlessly switch between the currently executed plan and the new plan, even during production. Further, this on-the-fly replanning capability is also applicable when newly identified production circumstances/objectives appear, such as a malfunctioning robot, material shortage, or a last-minute change to a customizable product. Another feature of the proposed MES solution is its distributed operation with multiple instances. Each instance can interpret its part of the production plan, dedicated to a location within the entire production site. All of these MES instances are continuously synchronized, and the actual global or partial (i.e., from the instance perspective) progress of the production is handled in real-time within one common digital twin. This article presents three main contributions: (i) an execution system that is capable of switching seamlessly between an original and a subsequently introduced alternative production plan, (ii) on-the-fly AI-powered planning and replanning of industrial production integrated into a digital twin, and (iii) a distributed MES, which allows for running multiple instances that may depend on topology or specific conditions of a real production plant. All of these outcomes are demonstrated and validated on a use-case utilizing an Industry 4.0 testbed, which is equipped with an automated transport system and several industrial robots. While our solution is tested on a lab-sized production system, the technological base is prepared to be scaled up to larger systems. © 2023 by the authors."
"MERLIN2: MachinEd Ros 2 pLanINg[Formula presented]","10.1016/j.simpa.2023.100477","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147924933&doi=10.1016%2Fj.simpa.2023.100477&partnerID=40&md5=7e2c268296374abd1ef66f1070cfdc24","Any service robot should be able to make decisions and schedule tasks to reach predefined goals such as opening a door or assisting users at home. However, these processes are not single short-term tasks anymore and it is required to set long-term skills for establishing a control architecture that allows robots to perform daily tasks. This paper presents MERLIN2, a hybrid cognitive architecture based on symbolic planning and state machine decision-making systems that allows performing robot behaviors. The architecture can run in any robot running ROS 2, the latest version of the Robot Operative System. MERLIN2 is available at https://github.com/MERLIN2-ARCH/merlin2. © 2023 The Author(s)"
"GraphMP: Graph Neural Network-based Motion Planning with Efficient Graph Search","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191193379&partnerID=40&md5=0ff6f7c1d48c4e1404778463d1e9ecf7","Motion planning, which aims to find a high-quality collision-free path in the configuration space, is a fundamental task in robotic systems. Recently, learning-based motion planners, especially the graph neural network-powered, have shown promising planning performance. However, though the state-of-the-art GNN planner can efficiently extract and learn graph information, its inherent mechanism is not well suited for graph search process, hindering its further performance improvement. To address this challenge and fully unleash the potential of GNN in motion planning, this paper proposes GraphMP, a neural motion planner for both low and high-dimensional planning tasks. With the customized model architecture and training mechanism design, GraphMP can simultaneously perform efficient graph pattern extraction and graph search processing, leading to strong planning performance. Experiments on a variety of environments, ranging from 2D Maze to 14D dual KUKA robotic arm, show that our proposed GraphMP achieves significant improvement on path quality and planning speed over state-of-the-art learning-based and classical planners; while preserving competitive success rate. © 2023 Neural information processing systems foundation. All rights reserved."
"Spatial Reasoning via Deep Vision Models for Robotic Sequential Manipulation","10.1109/IROS55552.2023.10342010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182523943&doi=10.1109%2FIROS55552.2023.10342010&partnerID=40&md5=2ce4a46a58351784da0fbcfe8bba868c","In this paper, we propose using deep neural architectures (i.e., vision transformers and ResNet) as heuristics for sequential decision-making in robotic manipulation problems. This formulation enables predicting the subset of objects that are relevant for completing a task. Such problems are often addressed by task and motion planning (TAMP) formulations combining symbolic reasoning and continuous motion planning. In essence, the action-object relationships are resolved for discrete, symbolic decisions that are used to solve manipulation motions (e.g., via nonlinear trajectory optimization). However, solving long-horizon tasks requires consideration of all possible action-object combinations which limits the scalability of TAMP approaches. To overcome this combinatorial complexity, we introduce a visual perception module integrated with a TAMP-solver. Given a task and an initial image of the scene, the learned model outputs the relevancy of objects to accomplish the task. By incorporating the predictions of the model into a TAMP formulation as a heuristic, the size of the search space is significantly reduced. Results show that our framework finds feasible solutions more efficiently when compared to a state-of-the-art TAMP solver. © 2023 IEEE."
"Learning Whom to Trust in Navigation: Dynamically Switching Between Classical and Neural Planning","10.1109/IROS55552.2023.10342308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174476561&doi=10.1109%2FIROS55552.2023.10342308&partnerID=40&md5=4323a41747991d78c5cbfd742b4c6c18","Navigation of terrestrial robots is typically addressed either with localization and mapping (SLAM) followed by classical planning on the dynamically created maps, or by machine learning (ML), often through end-to-end training with reinforcement learning (RL) or imitation learning (IL). Recently, modular designs have achieved promising results, and hybrid algorithms that combine ML with classical planning have been proposed. Existing methods implement these combinations with handcrafted functions, which cannot fully exploit the complementary nature of the policies and the complex regularities between scene structure and planning performance. Our work builds on the hypothesis that the strengths and weaknesses of neural planners and classical planners follow some regularities, which can be learned from training data, in particular from interactions. This is grounded on the assumption that, both, trained planners and the mapping algorithms underlying classical planning are subject to failure cases depending on the semantics of the scene and that this dependence is learnable: for instance, certain areas, objects or scene structures can be reconstructed easier than others. We propose a hierarchical method composed of a high-level planner dynamically switching between a classical and a neural planner. We fully train all neural policies in simulation and evaluate the method in both simulation and real experiments with a LoCoBot robot, showing significant gains in performance, in particular in the real environment. We also qualitatively conjecture on the nature of data regularities exploited by the high-level planner. © 2023 IEEE."
"Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning","10.3389/frobt.2023.1221739","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169314563&doi=10.3389%2Ffrobt.2023.1221739&partnerID=40&md5=ea391face746c6e11226740e8954fcb1","Long-horizon task planning is essential for the development of intelligent assistive and service robots. In this work, we investigate the applicability of a smaller class of large language models (LLMs), specifically GPT-2, in robotic task planning by learning to decompose tasks into subgoal specifications for a planner to execute sequentially. Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark. We compare our approach with classical planning and baseline methods to examine the applicability and generalizability of LLM-based planners. Our findings suggest that the knowledge stored in an LLM can be effectively grounded to perform long-horizon task planning, demonstrating the promising potential for the future application of neuro-symbolic planning methods in robotics. © © 2023 Chalvatzaki, Younes, Nandha, Le, Ribeiro and Gurevych."
"Multi-Object Navigation in real environments using hybrid policies","10.1109/ICRA48891.2023.10161030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168655908&doi=10.1109%2FICRA48891.2023.10161030&partnerID=40&md5=b145087b0cdb3b195612863cc35e7388","Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments. In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task [41] and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task [28]. © 2023 IEEE."
"Learning Neuro-symbolic Programs for Language Guided Robot Manipulation","10.1109/ICRA48891.2023.10160545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168652224&doi=10.1109%2FICRA48891.2023.10160545&partnerID=40&md5=02607420a03e0e3c83a069744ed9a69e","Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled action representations. Our experiments on a simulated environment with a 7-DOF manipulator, consisting of instructions with varying number of steps and scenes with different number of objects, demonstrate that our model is robust to such variations and significantly outperforms baselines, particularly in the generalization settings. The code, dataset and experiment videos are available at https://nsrmp.github.io © 2023 IEEE."
"Applications of Hybrid Conditional Planning in Service Robotics","10.1109/ICRAI57502.2023.10089569","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153565661&doi=10.1109%2FICRAI57502.2023.10089569&partnerID=40&md5=9f3861f1786c92a1723cd733d2ed77be","Hybrid planning computes task plans in full observability (a sequence of possible actuation actions) via the integration of low-level feasibility checks in classical planning approaches. Conditional planning extends the classical planning framework to account for plans under partial observability. We use a hybrid conditional planning approach to compute plans that include sensing and actuation actions in solving real-world problems by addressing uncertainties due to incomplete information at the planning phase. We generate a hybrid conditional plan of actions using parallel instances of a non-monotonic hybrid HCPLAN that supports defaults, integration of external computations, and non-deterministic actions. We show our hybrid conditional planning framework's HCPLAN applicability through service robotics scenarios with manipulation and navigation tasks. Furthermore, we evaluate the effect of parallel threads on the computation of hybrid conditional framework on different benchmark scenarios for the service robotics domains. © 2023 IEEE."
"Oracle-SAGE: Planning Ahead in Graph-Based Deep Reinforcement Learning","10.1007/978-3-031-26412-2_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151052294&doi=10.1007%2F978-3-031-26412-2_4&partnerID=40&md5=7e7e66df493c780b2fbea749bc2a0ea5","Deep reinforcement learning (RL) commonly suffers from high sample complexity and poor generalisation, especially with high-dimensional (image-based) input. Where available (such as some robotic control domains), low dimensional vector inputs outperform their image based counterparts, but it is challenging to represent complex dynamic environments in this manner. Relational reinforcement learning instead represents the world as a set of objects and the relations between them; offering a flexible yet expressive view which provides structural inductive biases to aid learning. Recently relational RL methods have been extended with modern function approximation using graph neural networks (GNNs). However, inherent limitations in the processing model for GNNs result in decreased returns when important information is dispersed widely throughout the graph. We outline a hybrid learning and planning model which uses reinforcement learning to propose and select subgoals for a planning model to achieve. This includes a novel action selection mechanism and loss function to allow training around the non-differentiable planner. We demonstrate our algorithms effectiveness on a range of domains, including MiniHack and a challenging extension of the classic taxi domain. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG."
"SGL: Symbolic Goal Learning in a Hybrid, Modular Framework for Human Instruction Following","10.1109/LRA.2022.3190076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134260942&doi=10.1109%2FLRA.2022.3190076&partnerID=40&md5=b2b4b922c4da6ebf60c7f5dfef545278","This paper investigates human instruction following for robotic manipulation via a hybrid, modular system with symbolic and connectionist elements. Symbolic methods build modular systems with semantic parsing and task planning modules for producing sequences of actions from natural language requests. Modern connectionist methods employ deep neural networks that learn visual and linguistic features for mapping inputs to a sequence of low-level actions, in an end-to-end fashion. The hybrid, modular system blends these two approaches to create a modular framework: it formulates instruction following as symbolic goal learning via deep neural networks followed by task planning via symbolic planners. Connectionist and symbolic modules are bridged with Planning Domain Definition Language. The vision-and-language learning network predicts its goal representation, which is sent to a planner for producing a task-completing action sequence. For improving the flexibility of natural language, we further incorporate implicit human intents with explicit human instructions. To learn generic features for vision and language, we propose to separately pretrain vision and language encoders on scene graph parsing and semantic textual similarity tasks. Benchmarking evaluates the impacts of different components of, or options for, the vision-and-language learning model and shows the effectiveness of pretraining strategies. Manipulation experiments conducted in the simulator AI2THOR show the robustness of the framework to novel scenarios. © 2022 IEEE."
"Analyzing generalized planning under nondeterminism","10.1016/j.artint.2022.103696","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126905079&doi=10.1016%2Fj.artint.2022.103696&partnerID=40&md5=f25424627fa669b915e2948875f7860d","In automated planning, there has been a recent interest in solving a class of problems, where a single solution applies for multiple, possibly infinitely many, instances. This necessitates a generalized notion of plans, such as plans with loops. However, the correctness of such plans is non-trivial to define, making it difficult to provide a clear specification of what we should be looking for. In an influential paper, Levesque proposed a formal specification for analyzing the correctness of such plans. He motivated a logical characterization within the situation calculus that included binary sensing actions. This characterization argued that from each state considered possible initially, the plan should terminate while satisfying the goal. Increasingly, classical plan structures are being applied to stochastic environments such as robotics applications. This raises the question as to what the specification for correctness should look like, since Levesque's account makes the assumption that actions are deterministic. In this work, we aim to generalize Levesque's account to handle actions with nondeterministic outcomes, which may also be accorded probabilities. By appealing to an extension of the situation calculus to handle probabilistic nondeterminism, we will show that Levesque's definition, as well as a notion of goal achievability proposed by Lin and Levesque, have limited appeal under stochastic nondeterminism. In essence, they correspond to one correct execution, which is unlikely to be adequate. Rather, we propose to delineate between goal satisfaction and termination leading to a range of correctness criteria. To better study these criteria, and to position the results in a broader context while still allowing for the generality of the situation calculus, we consider an abstract framework to study the correctness of plans with loops, in domains that are possibly unbounded, and/or stochastic, and/or continuous. Within that framework, we then prove numerous relationships between the criteria, including some impossibility results for categorically satisfying goals. Finally, we show that these notions provide a more granular view than those discussed in the literature, such as strong planning and strong cyclic planning. © 2022 Elsevier B.V."
"Hybrid Model-Based Emotion Contextual Recognition for Cognitive Assistance Services","10.1109/TCYB.2020.3013112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130767915&doi=10.1109%2FTCYB.2020.3013112&partnerID=40&md5=aa8185ea6eaf70abcc4b52bd855c6d8d","Endowing ubiquitous robots with cognitive capabilities for recognizing emotions, sentiments, affects, and moods of humans in their context is an important challenge, which requires sophisticated and novel approaches of emotion recognition. Most studies explore data-driven pattern recognition techniques that are generally highly dependent on learning data and insufficiently effective for emotion contextual recognition. In this article, a hybrid model-based emotion contextual recognition approach for cognitive assistance services in ubiquitous environments is proposed. This model is based on: 1) a hybrid-level fusion exploiting a multilayer perceptron (MLP) neural-network model and the possibilistic logic and 2) an expressive emotional knowledge representation and reasoning model to recognize nondirectly observable emotions; this model exploits jointly the emotion upper ontology (EmUO) and the n-ary ontology of events HTemp supported by the NKRL language. For validation purposes of the proposed approach, experiments were carried out using a YouTube dataset, and in a real-world scenario dedicated to the cognitive assistance of visitors in a smart devices showroom. Results demonstrated that the proposed multimodal emotion recognition model outperforms all baseline models. The real-world scenario corroborates the effectiveness of the proposed approach in terms of emotion contextual recognition and management and in the creation of emotion-based assistance services. © 2013 IEEE."
"Learning an Explainable Trajectory Generator Using the Automaton Generative Network (AGN)","10.1109/LRA.2021.3135940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121842278&doi=10.1109%2FLRA.2021.3135940&partnerID=40&md5=2dd2c131f626ce22fb42f2e8c49fa67f","Symbolic reasoning is a key component for enabling practical use of data-driven planners in autonomous driving. In that context, deterministic finite state automata (DFA) are often used to formalize the underlying high-level decision-making process. Manual design of an effective DFA can be tedious. In combination with deep learning pipelines, DFA can serve as an effective representation to learn and process complex behavioral patterns. The goal of this work is to leverage that potential. We propose the automaton generative network (AGN), a differentiable representation of DFAs. The resulting neural network module can be used standalone or as an embedded component within a larger architecture. In evaluations on deep learning based autonomous vehicle planning tasks, we demonstrate that incorporating AGN improves the explainability, sample efficiency, and generalizability of the model. © 2016 IEEE."
"A System For Robot Concept Learning Through Situated Dialogue","10.18653/v1/2022.sigdial-1.64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182517095&doi=10.18653%2Fv1%2F2022.sigdial-1.64&partnerID=40&md5=e94feaa0d773b5c0169281d6b4907da4","Robots operating in unexplored environments with human teammates will need to learn unknown concepts on the fly. To this end, we demonstrate a novel system that combines a computational model of question generation with a cognitive robotic architecture. The model supports dynamic production of back- and-forth dialogue for concept learning given observations of an environment, while the architecture supports symbolic reasoning, action representation, one-shot learning and other capabilities for situated interaction. The system is able to learn about new concepts including objects, locations, and actions, using an underlying approach that is generalizable and scalable. We evaluate the system by comparing learning efficiency to a human baseline in a collaborative reference resolution task and show that the system is effective and efficient in learning new concepts, and that it can informatively generate explanations about its behavior. © 2022 Association for Computational Linguistics."
"Learning Efficient Multi-agent Cooperative Visual Exploration","10.1007/978-3-031-19842-7_29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142674934&doi=10.1007%2F978-3-031-19842-7_29&partnerID=40&md5=bd49a9ae1fa097f89c3393e6045b36dd","We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we propose a novel RL-based multi-agent planning module, Multi-agent Spatial Planner (MSP). MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG."
"Online Grounding of Symbolic Planning Domains in Unknown Environments","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141708401&partnerID=40&md5=3eb1f4f7619f51cecc03b49f4c56e4b2","If a robotic agent wants to exploit symbolic planning techniques to achieve some goal, it must be able to properly ground an abstract planning domain in the environment in which it operates. However, if the environment is initially unknown by the agent, the agent needs to explore it and discover the salient aspects of the environment necessary to reach its goals. Namely, the agent has to discover: (i) the objects present in the environment, (ii) the properties of these objects and their relations, and finally (iii) how abstract actions can be successfully executed. The paper proposes a framework that aims to accomplish the aforementioned perspective for an agent that perceives the environment partially and subjectively, through real value sensors (e.g., GPS, and on-board camera) and can operate in the environment through low level actuators (e.g., move forward of 20 cm). We evaluate the proposed architecture in photo-realistic simulated environments, where the sensors are RGB-D on-board camera, GPS and compass, and low level actions include movements, grasping/releasing objects, and manipulating objects. The agent is placed in an unknown environment and asked to find objects of a certain type, place an object on top of another, close or open an object of a certain type. We compare our approach with a state of the art method on object goal navigation based on reinforcement learning, showing better performances. © 19th International Conference on Principles of Knowledge Representation and Reasoning, KR 2022. All rights reserved."
"Non-Axiomatic Term Logic: A Theory of Cognitive Symbolic Reasoning","10.1527/tjsai.37-6_C-M11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140923328&doi=10.1527%2Ftjsai.37-6_C-M11&partnerID=40&md5=b848a72c036e2f8a8d0dd0eea87faece","This paper presents Non-Axiomatic Term Logic (NATL) as a theoretical computational framework of human-like symbolic reasoning in artificial intelligence. NATL unites a discrete syntactic system inspired from Aristotle’s term logic and a continuous semantic system based on the modern idea of distributed representations, or embeddings. This paper positions the proposed approach in the phylogeny and the literature of logic, and explains the framework. As it is yet no more than a theory and it requires much further elaboration to implement it, no quantitative evaluation is presented. Instead, qualitative analyses of arguments using NATL, some applications to possible cognitive science/robotics-related research, and remaining issues towards a machinery implementation are discussed. © 2022, Japanese Society for Artificial Intelligence. All rights reserved."
"Epistemic Multiagent Reasoning with Collaborative Robots","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138309180&partnerID=40&md5=dcf16186bdad21b838b7cd875126a253","Over the last few years, the fields of Artificial Intelligence, Robotics and IoT have gained a lot of attention. This increasing interest has brought, among other things, to the development of autonomous multi-agent systems where robotic entities may interact with each other. As for all the other autonomous settings, also these systems require arbitration. Our work tries to address this problem by presenting a framework that embeds both a classical and a multi-agent epistemic (epistemic, for brevity) planner in a robotic control architecture. The idea is to combine the (i) classical and the (ii) epistemic solvers to model efficiently the interaction with: the (i) physical world and the (ii) information flows, respectively. In particular, the presented architecture starts by planning on the “epistemic level"" refining then single-agent world-altering actions thanks to the classical planner. To further optimize the solving process, we also introduce the concept of macros in epistemic planning. Macros, in fact, have been successfully employed in classical planning as they allow for opportune aggregations of actions that may lead to a reduction of plans' length. Finally, the overall framework is exemplified and validated with two Franka Emika manipulators. This allowed us to empirically justify how the combination of the two planning approaches (classical and epistemic), and the introduction of macros, reduce the computational time required by the orchestrating phase. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)"
"A Hybrid Planning Approach for Accompanying Information-gathering in Plan Execution Monitoring","10.1007/s10846-021-01480-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113600856&doi=10.1007%2Fs10846-021-01480-5&partnerID=40&md5=88816697bb76edec174f50389aecdb53","Autonomous robots that execute plans in real-world environments may easily get failed due to unexpected environment dynamics. For robust plan execution, a great number of plan execution monitoring works has been proposed to estimate plan execution effects and make recovery plans for any detected plan failures. However, most works have assumed the full observability of environment states and accessibility of complete information about environment dynamics, which shows limitations when executing plans under environment dynamics and partial observability. To efficiently and effectively monitor robot plans in dynamic and partially observable environments, this paper first proposes an accompanying action policy that specifies an interaction pattern between information-gathering plan and the task plan to estimate and monitor task plan execution. To provide a concrete implementation of the above pattern, we first identify the task achievement and execution monitoring as two separated goals in a robot task, and then propose a hybrid planning approach that integrates two planners to solve the goals. For a task-achievement goal, we utilize a classical planning framework ROSPlan to efficiently compute the global task plan. While executing each action of the plan, we cast the execution monitoring problem as a Partially Observable Markov Decision Process (POMDP), which plans the information-gathering plans for monitors the action execution effects and recover failures when necessary. We further implement a typical robotic service task to verify the efficiency and effectiveness of our approach. By comparing with both the baseline plan-invariant execution monitoring approach and a full POMDP planning approach, the hybrid planning approach is efficient in task planning and execution monitoring and effectively recovering plan failures in unknown environments. © 2021, The Author(s), under exclusive licence to Springer Nature B.V."
"Toward Creative Problem Solving Agents: Action Discovery through Behavior Babbling","10.1109/ICDL49984.2021.9515658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114557967&doi=10.1109%2FICDL49984.2021.9515658&partnerID=40&md5=25167e46778f667d7b50142a5fdb322d","Creative problem solving (CPS) is the process by which an agent discovers unknown information about itself and its environment, allowing it to accomplish a previously impossible goal. We propose a framework for CPS by robots for discovering novel actions via behavior babbling, capable of learning a representation of novel actions at both a symbolic planning level, and a sub-symbolic action controller level. Our framework employs two modes of discovery - a focused incubation method that scopes its search to the actions and entities composing the failed plan, and a defocused incubation method which enables exploration of actions and entities outside of the failed plan. We implemented and tested our framework using a Baxter robot in a 3D physics-based simulation environment, where we ran three proof-of-concept object manipulation scenarios. Results suggest that it is possible to use behavior babbling as a method for the autonomous discovery of flexible and reusable actions. © 2021 IEEE."
"Autonomous Robot Planning System for In-Space Assembly of Reconfigurable Structures","10.1109/AERO50100.2021.9438257","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111403236&doi=10.1109%2FAERO50100.2021.9438257&partnerID=40&md5=709a9169093419e79ee28b5d5e3b1f90","Large-scale space structures, such as telescopes or spacecrafts, require suitable in-situ assembly technologies in order to overcome the limitations on payload size and mass of current launch vehicles. In many application scenarios, manual assembly by astronauts is either highly cost-inefficient or not feasible at all due to orbital constraints. However, (semi-) autonomous robotic assembly systems may provide the means to construct larger structures in space in the near future. Modularity is a key concept for such structures, and also for reducing costs in novel spacecraft designs. The advantage of the modular approach lies in the capability to generate a high number of unique assets from a reduced number of building blocks. Thus, spacecrafts can be easily adapted to particular use cases, and could even be reconfigured during their lifetime using a robotic manipulation system. These ideas lie at the core of our current EU project MOSAR (MOdular Spacecraft Assembly and Reconfiguration). Teleoperating a space robotic system from Earth to assemble a modular structure is not straightforward. Major difficulties are related to time delays, communication losses, limited control modalities, and low immersion for the operator. Autonomous robotic operations are then preferred, and with this goal we propose a fully autonomous system for planning in-space assembly tasks. Our system is able to generate assembly and reconfiguration plans for modular structures in terms of high-level actions that can autonomously be executed by a robot. Through multiple simulation layers, the system automatically verifies the feasibility and correctness of action sequences created by the planner. The layers implement different levels of abstraction, hierarchically stacked to detect infeasible transitions and initiate replanning at an early stage. Levels of abstraction increase in complexity, ranging from a basic geometric description of the spacecraft, over kinematics of the robotic setup, to full representations of the actions. The system reuses information from failed checks in all layers to avoid similar situations during replanning. We use a hybrid approach where symbolic reasoning is combined with considerations of physical constraints to generate a holistic sequence of actions. We demonstrate our planner for large space structures in a simulation environment. In particular, we consider the reconfiguration of a given modular structure, i.e. disassemble parts and reassemble them in a new configuration. The adaptability of our planning system is shown by executing the assembly plans on robots with different sets of skills and in scenarios with simulated hardware failures. © 2021 IEEE."
"Hybrid conditional planning for robotic applications","10.1177/0278364920963783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093846385&doi=10.1177%2F0278364920963783&partnerID=40&md5=c1b75fe7059eb6f6ee1ef073e92a1682","Robots who have partial observability of and incomplete knowledge about their environments may have to consider contingencies while planning, and thus necessitate cognitive abilities beyond classical planning. Moreover, during planning, they need to consider continuous feasibility checks for executability of the plans in the real world. Conditional planning is concerned with reaching goals from an initial state, in the presence of incomplete knowledge and partial observability, by considering all contingencies and by utilizing sensing actions to gather relevant knowledge when needed. A conditional plan is essentially a tree of actions where each branch of the tree represents a possible execution of actuation actions and sensing actions to reach a goal state. Hybrid conditional planning extends conditional planning by integrating feasibility checks into executability conditions of actions. We introduce a parallel offline algorithm, called HCPlan, for computing hybrid conditional plans. HCPlan relies on modeling deterministic effects of actuation actions and non-deterministic effects of sensing actions in the causality-based action language (Formula presented.). Branches of a hybrid conditional plan are computed in parallel using a SAT solver, where continuous feasibility checks are performed as needed. We develop a comprehensive benchmark suite and introduce new evaluation metrics for hybrid conditional planning. We evaluate HCPlan with extensive experiments in terms of computational efficiency and plan quality. We perform experiments to compare HCPlan with other related conditional planners and approaches to deal with contingencies due to incomplete knowledge. We further demonstrate the applicability and usefulness of HCPlan in service robotics applications, through dynamic simulations and physical implementations. © The Author(s) 2020."
"Fully automatic data collection for neuro-symbolic task planning for mobile robot navigation","10.1109/SMC52423.2021.9658822","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124253948&doi=10.1109%2FSMC52423.2021.9658822&partnerID=40&md5=184b6b5eea522845df53d1ed45a6bf07","In this paper, we present an automatic collection method of image data for neuro-symbolic task planning for robot navigation. Collecting images for robot task planning would often be overwhelmed by laborious chores to operate the robot, change the environment, and repeatedly capture quality-assured images. We propose a method using a robotic simulator to perform a series of repetitive processes for data collection automatically. It generates (i) a random instance of the navigation problem, (ii) a simulation environment that depicts the instance, (iii) a planning problem instance described in a classical planning language, (iv) a task plan that solves the planning problem, (v) control inputs for the robot to execute the task plan, and (vi) a sequence of cropped images capturing the evolving states of the robot and the world while the robot performs the plan. We use one of the state-of-the-art neuro-symbolic planning models to validate our method. From the evaluation, the model achieves at most 92.6% of the success rate in generating task plans successfully from only a pair of images showing the initial and the desired state. © 2021 IEEE."
"Dynamic action selection using image schema-based reasoning for robots","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117035116&partnerID=40&md5=6b4cf688c3237fd72c034ab48ceb1544","Dealing with robotic actions in uncertain environments has been demonstrated to be hard. Many classic planning approaches to robotic action make the closed world assumption, rendering them inefficient for everyday household activities, as they function without generalizability to other contexts or the ability to deal with unexpected changes. In contrast, humans robustly execute underspecified instructions in unfamiliar environments. In this paper, we initiate our research program where we propose the use of functional relations in the form of image-schematic micro-theories, formally represented in ISLFOL, to enrich action descriptors with semantic components. It builds on the body of work in embodied cognition showing that human conceptualization of action sequences is founded on abstract patterns learned from physical experiences in the form of spatiotemporal relationships between object, agents and environments. These theories are used to inform action selection mechanisms for behavioral robotics written in EL++ and we argue how these micro-patterns can be applied in a more general way to deal with underspecified action commands and commonsense problem-solving. © 2021 CEUR-WS. All rights reserved."
"UAV remote sensing-based phenotyping to evaluate drought stress in turfgrass","10.13031/aim.202100654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114200390&doi=10.13031%2Faim.202100654&partnerID=40&md5=f3e8e113ba6f928d295366196749993d","Breeding turfgrasses using classical plant breeding methods is a long-term process which requires a breeder to sort through large segregating populations by phenotypically evaluating the plants across multiple environments and over several years. The quality and frequency of phenotypic data collection at a field-scale is currently the bottleneck limiting the efficiency and accuracy of classical phenotype-based breeding. The use of unmanned aerial vehicle (UAV) remote sensing has proven to be viable method to collect geospatial data rapidly and at fine spatial and high temporal scales in major agricultural crops. In this study, UAV remote sensing and machine learning algorithms were utlilzed to evaluate drought stress on zoysiagrass (Zoysia spp.) breeding nursery planted in 2017. Selections of the top 2% best-performing hybrids under drought stress were made using visual parameters (turfgrass quality under normal and drydown conditions) as well as UAV-derived NDVI (normalized difference vegetation index). The results suggest an agreement between the conventional selection methods and proposed UAV phenotyping approaches. © ASABE 2021 Annual International Meeting"
"Towards Explainable Visionary Agents: License to Dare and Imagine","10.1007/978-3-030-82017-6_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113337069&doi=10.1007%2F978-3-030-82017-6_9&partnerID=40&md5=a530422dc76d0918d854ee8f7c4ed628","Since their appearance, computer programs have embodied discipline and structured approaches and methodologies. Yet, to this day, equipping machines with imaginative and creative capabilities remains one of the most challenging and fascinating goals we pursue. Intelligent software agents can behave intelligently in well-defined scenarios, relying on Machine Learning (ML), symbolic reasoning, and the ability of their developers for tailoring smart behaviors to specific application domains. However, to forecast the evolution of all possible scenarios is unfeasible. Thus, intelligent agents should autonomously/creatively adapt to the world’s mutability. This paper investigates the meaning of imagination in the context of cognitive agents. In particular, it addresses techniques and approaches to let agents autonomously imagine/simulate their course of action and generate explanations supporting it, and formalizes thematic challenges. Accordingly, we investigate research areas including: (i) reasoning and automatic theorem proving to synthesize novel knowledge via inference; (ii) automatic planning and simulation, used to speculate over alternative courses of action; (iii) machine learning and data mining, exploited to induce new knowledge from experience; and (iv) biochemical coordination, which keeps imagination dynamic by continuously reorganizing it. © 2021, Springer Nature Switzerland AG."
"Planning with Learned Object Importance in Large Problem Instances using Graph Neural Networks","10.1609/aaai.v35i13.17421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108292900&doi=10.1609%2Faaai.v35i13.17421&partnerID=40&md5=f470e9a9727e7b746ffd931bee7e750e","Real-world planning problems often involve hundreds or even thousands of objects, straining the limits of modern planners. In this work, we address this challenge by learning to predict a small set of objects that, taken together, would be sufficient for finding a plan. We propose a graph neural network architecture for predicting object importance in a single inference pass, thus incurring little overhead while greatly reducing the number of objects that must be considered by the planner. Our approach treats the planner and transition model as black boxes, and can be used with any off-the-shelf planner. Empirically, across classical planning, probabilistic planning, and robotic task and motion planning, we find that our method results in planning that is significantly faster than several baselines, including other partial grounding strategies and lifted planners. We conclude that learning to predict a sufficient set of objects for a planning problem is a simple, powerful, and general mechanism for planning in large instances. Video: https://youtu.be/FWsVJc2fvCE Code: https://git.io/JIsqX. © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"A neural-symbolic approach for user mental modeling: A step towards building exchangeable identities","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104644928&partnerID=40&md5=ebd328f3bb3c8773ea5d6d72bb9928eb","Combining symbolic-reasoning and data learning in a unified double-loop learning system can contribute to the emergence of artificial intelligence solutions that are more adaptive to social and behavioural context. This paper presents a hybrid user modeling framework that relies on the integration of machine learning and reasoning methods equipped with formally represented domain knowledge. We find that this approach contributes to the design of context-aware systems that require less data, manage bias better, provide better transparency and can handle data sparsity more effectively. We present the impact of our work in different social domains from building trusted digital surrogates to decentralization of social recommendation services. Our approach can construct software agents from identity and expertise of users and allows such entities to become more digitally portable. Our approach also contributes to the emergence of expertise sharing paradigms that are less prone to biases and more privacy preserving. The paper uses these domain applications to validate the scalability and versatility of our approach augmented with principles of open and transparent algorithms. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)"
"A knowledge-intensive adaptive business process management framework","10.1016/j.is.2020.101639","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091035439&doi=10.1016%2Fj.is.2020.101639&partnerID=40&md5=573aa6cf8c2147f416be424a70ce5a7e","Business process management has been the driving force of optimization and operational efficiency for companies until now, but the digitalization era we have been experiencing requires businesses to be agile and responsive as well. In order to be a part of this digital transformation, delivering new levels of automation-fueled agility through digitalization of BPM itself is required. However, the automation of BPM cannot be achieved by solely focusing on process space and classical planning techniques. It requires a holistic approach that also captures the social aspects of the business environment, such as corporate strategies, organization policies, negotiations, and cooperation. For this purpose, we combine BPM, knowledge-intensive systems and intelligent agent technologies, and yield one consolidated intelligent business process management framework, namely agileBPM, that governs the entire BPM life-cycle. Accordingly, agileBPM proposes a modeling methodology to semantically capture the business interests, enterprise environment and process space in accordance with the agent-oriented software engineering paradigm. The proposed agent-based process execution environment provides cognitive capabilities (such as goal-driven planning, norm compliance, knowledge-driven actions, and dynamic cooperation) on top of the developed business models to support knowledge workers’ multi-criteria decision making tasks. The context awareness and exception handling capabilities of the proposed approach have been presented with experimental studies. Through comparative evaluations, it is shown that agileBPM is the most comprehensive knowledge-intensive process management solution. © 2020"
"An automated planning model for hri: Use cases on social assistive robotics","10.3390/s20226520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096032575&doi=10.3390%2Fs20226520&partnerID=40&md5=cd35ef83bc473ba66d5ca03c6b0d7bd5","Using Automated Planning for the high level control of robotic architectures is becoming very popular thanks mainly to its capability to define the tasks to perform in a declarative way. However, classical planning tasks, even in its basic standard Planning Domain Definition Language (PDDL) format, are still very hard to formalize for non expert engineers when the use case to model is complex. Human Robot Interaction (HRI) is one of those complex environments. This manuscript describes the rationale followed to design a planning model able to control social autonomous robots interacting with humans. It is the result of the authors’ experience in modeling use cases for Social Assistive Robotics (SAR) in two areas related to healthcare: Comprehensive Geriatric Assessment (CGA) and non-contact rehabilitation therapies for patients with physical impairments. In this work a general definition of these two use cases in a unique planning domain is proposed, which favors the management and integration with the software robotic architecture, as well as the addition of new use cases. Results show that the model is able to capture all the relevant aspects of the Human-Robot interaction in those scenarios, allowing the robot to autonomously perform the tasks by using a standard planning-execution architecture. © 2020 by the authors. Licensee MDPI, Basel, Switzerland."
"Task-Oriented Multi-Modal Question Answering for Collaborative Applications","10.1109/ICIP40778.2020.9190659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098645383&doi=10.1109%2FICIP40778.2020.9190659&partnerID=40&md5=ff14a2cae7876e24e7ed8b2f33167164","Cobots that can work in human workspaces and adapt to human need to understand and respond to human's inquiry and instruction. In this paper, we propose new question answering (QA) task and dataset for human-robot collaboration on task-oriented operation, i.e., task-oriented collaborative QA (TCQA). Differing from conventional video QA for answering questions about what happened in video clips constrained by scripts and subtitles, TC-QA aims to share common ground for task-oriented operation through question answering. We propose an open-end (OE) format of answer with text reply, image with annotated related objects, and video with operation duration to guide operation execution. Designed for grounding, the TC-QA dataset comprises query videos and questions to seek acknowledgement, correction, attention to task-related objects, and information on objects or operation. Due to the flexibility of real-world task with limited training sample, we propose and evaluate a baseline method based on a hybrid approach. The hybrid approach employs deep learning methods for object detection, hand detection and gesture recognition, and symbolic reasoning to ground question on observation for providing the answer. Our experiments show that the hybrid method is effective for the TC-QA task. © 2020 IEEE."
"Manipulation Planning Using Object-Centered Predicates and Hierarchical Decomposition of Contextual Actions","10.1109/LRA.2020.3009063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089894881&doi=10.1109%2FLRA.2020.3009063&partnerID=40&md5=ecf002289a92505a6ec7fb5b435d4360","Current approaches combining task and motion planning require intensive geometric and symbolic reasoning to find feasible motions for task execution. The poor expressiveness of task planning domains for characterizing geometric changes with actions and the difficulties faced by current approaches to efficiently identify motion dependencies for plan execution produce expensive callings to motion planning on unfeasible actions and intensive reasoning to find realizable plans. In this work we combine two recent approaches to address these problems. Task planning is carried out using an object-centered description of geometric relations that consistently characterizes changes in the object configuration space. Plan execution is implemented using a symbol to motion hierarchical decomposition that depends on consecutive actions in the plan, rather than on single actions, which permits considering motion dependencies across plan actions for a successful execution. © 2016 IEEE."
"Neuro-Symbolic Program Search for Autonomous Driving Decision Module Design","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102734355&partnerID=40&md5=a56e386eade3ceb74449d459b2343229","As a promising topic in cognitive robotics, neuro-symbolic modeling integrates symbolic reasoning and neural representation altogether. However, previous neuro-symbolic models usually wire their structures and the connections manually, making the underlying parameters sub-optimal. In this work, we propose the Neuro-Symbolic Program Search (NSPS) to improve the autonomous driving system design. NSPS is a novel automated search method that synthesizes the Neuro-Symbolic Programs. It can produce robust and expressive Neuro-Symbolic Programs and automatically tune the hyper-parameters. We validate NSPS in the CARLA driving simulation environment. The resulting Neuro-Symbolic Decision Programs successfully handle multiple traffic scenarios. Compared with previous neural-network-based driving and rule-based methods, our neuro-symbolic driving pipeline achieves more stable and safer behaviors in complex driving scenarios while maintaining an interpretable symbolic decision-making process. © 2020 Proceedings of Machine Learning Research. All rights reserved."
"A Software Architecture for Service Robots Manipulating Objects in Human Environments","10.1109/ACCESS.2020.3003991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090248501&doi=10.1109%2FACCESS.2020.3003991&partnerID=40&md5=51d3476cb41acf71516d006a07e40be7","This paper presents a software architecture for robots providing manipulation services autonomously in human environments. In an unstructured human environment, a service robot often needs to perform tasks even without human intervention and prior knowledge about tasks and environments. For autonomous execution of tasks, varied processes are necessary such as perceiving environments, representing knowledge, reasoning with the knowledge, and planning for task and motion. While developing each of the processes is important, integrating them into a working system for deployment is also important as a robotic system can bring tangible outcomes when it works in real world. However, such an architecture has been rarely realized in the literature owing to the difficulties of a full integration, deployment, understanding high-level goals without human interventions. In this work, we suggest a software architecture that integrates the components necessary to perform tasks by a real robot without human intervention. We show our architecture composed of deep learning based perception, symbolic reasoning, AI task planning, and geometric motion planning. We implement a deep neural network that produces information about the environment, which are then stored in a knowledge base. We implement a reasoner that processes the knowledge to use the result for task planning. We show our implementation of the symbolic task planner that generates a sequence of motion predicates. We implement an interface that computes geometric information necessary for motion planning to execute the symbolic task plans. We describe the deployment of the architecture through the result of lab tests and a public demonstration. The architecture is developed based on Robot Operating System (ROS) so compatible with any robot that is capable of object manipulation and mobile navigation running in ROS. We deploy the architecture to two different robot platforms to show the compatibility. © 2013 IEEE."
"Semantic visual recognition in a cognitive architecture for social robots","10.3233/ICA-200624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085245167&doi=10.3233%2FICA-200624&partnerID=40&md5=4407598e8042a118381fa768f01e9b34","Cognitive architectures allow robots to perform their operations by drawing on a process that aims to simulate human reasoning. This paper presents an integrated semantic artificial memory system in cognitive architecture based on symbolic reasoning and a connective representation of the knowledge. This memory system attempts to simulate how humans learn to distinguish instances of particular objects within their class using a convolutional network to detect the relevant elements of an image. We use a vector with the extracted features to learn to discriminate an instance of another element from the same class. A novel feature of our approach is its autonomous learning process during the operation of the robot, integrating a deep learning embedding with a statistical classifier. The usefulness and robustness of this method are demonstrated by applying it to a social robot that learns to differentiate people. Finally, experiments are carried out to validate our approach, comparing the detection results with several alternative methods. © 2020 - IOS Press and the authors. All rights reserved."
