"Source title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Self-Adaptive Multi-Agent System Approach for Collaborative Mobile Learning","D. G. de la Iglesia; J. F. Calderón; D. Weyns; M. Milrad; M. Nussbaum","Media Technology, Linnaeus University, Växjö, Kronoberg, Sweden; Departamento de Ciencia de la Computacion, Escuela de Ingeniera PUC, Santiago, RM, Chile; Department of Computer Science, Linnaeus University, DFM, Småland, Sweden; Center for Learning and Knowledge Technologies (CeLeKT), Linnaeus University, Vaxjo, Kronoberg, Sweden; Departamento de Ciencia de la Computacion, Escuela de Ingeniera PUC, RM, Chile",IEEE Transactions on Learning Technologies,"20 May 2017","2015","8","2","158","172","Mobile technologies have emerged as facilitators in the learning process, extending traditional classroom activities. However, engineering mobile learning applications for outdoor usage poses severe challenges. The requirements of these applications are challenging, as many different aspects need to be catered, such as resource access and sharing, communication between peers, group management, activity flow, etc. Robustness is particularly important for learning scenarios to guarantee undisturbed and smooth user experiences, pushing the technological aspects in the background. Despite significant research in the field of mobile learning, very few efforts have focused on collaborative mobile learning requirements from a software engineering perspective. This paper focuses on aspects of the software architecture, aiming to address the challenges related to resource sharing in collaborative mobile learning activities. This includes elements such as autonomy for personal interactive learning, richness for large group collaborative learning (indoor and outdoor), as well as robustness of the learning system. Additionally, we present self-adaptation as a solution to mitigate risks of resource unavailability and organization failures that arise from environment and system dynamism. Our evaluation provides indications regarding the system correctness with respect to resource sharing and collaboration concerns, and offers qualitative evidence of self-adaptation benefits for collaborative mobile learning applications.","1939-1382","","10.1109/TLT.2014.2367493","Center for Research on Educa-tional Policy and Practice(grant numbers:CIE01-CONICYT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6948376","Mobile Learning;Software Architecture;Multi-Agent Systems;Self-Adaptation;Mobile learning;software architecture;multi-agent systems;self-adaptation","Mobile communication;Collaboration;Mobile handsets;Software;Robustness;Computer architecture;Performance evaluation","","28","","45","IEEE","5 Nov 2014","","","IEEE","IEEE Journals"
"A Novel Multi-Agent Parallel-Critic Network Architecture for Cooperative-Competitive Reinforcement Learning","Y. Sun; J. Lai; L. Cao; X. Chen; Z. Xu; Y. Xu","Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; The PLA Unit 31102, Nanjing, China",IEEE Access,"31 Jul 2020","2020","8","","135605","135616","Multi-agent deep reinforcement learning (MDRL) is an emerging research hotspot and application direction in the field of machine learning and artificial intelligence. MDRL covers many algorithms, rules and frameworks, it is currently researched in swarm system, energy allocation optimization, stocking analysis, sequential social dilemma, and with extremely bright future. In this paper, a parallel-critic method based on classic MDRL algorithm MADDPG is proposed to alleviate the training instability problem in cooperative-competitive multi-agent environment. Furthermore, a policy smoothing technique is introduced to our proposed method to decrease the variance of learning policies. The suggested method is evaluated in three different scenarios of authoritative multi-agent particle environment (MPE). Multiple statistical data of experimental results show that our method significantly improves the training stability and performance compared to vanilla MADDPG.","2169-3536","","10.1109/ACCESS.2020.3011670","National Natural Science Foundation of China(grant numbers:61806221); Advanced research fund of equipment development department(grant numbers:61421010318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146854","Multi-agent system;deep reinforcement learning;parallel-critic architecture;training stability","Training;Task analysis;Machine learning;Stability analysis;Games;Network architecture;Learning (artificial intelligence)","","12","","45","CCBY","24 Jul 2020","","","IEEE","IEEE Journals"
"Rational Coordination in Cognitive Agents: A Decision-Theoretic Approach Using ERMM","N. Mazhar; M. Kausar","Department of Software Engineering, Foundation University Islamabad, Islamabad, Pakistan; Department of Software Engineering, Foundation University Islamabad, Islamabad, Pakistan",IEEE Access,"1 Sep 2023","2023","11","","92628","92646","Over the years, research in multi-agent systems has become increasingly popular. Agents evolve by interacting with their environment and must communicate with other agents in order to do various cooperative tasks. The research aims to provide efficient coordination among cooperative cognitive agents in unpredictable multi-agent situations. Xiang’s rational agent model addresses scenarios when no social conventions or predefined communication protocols exist for the agents’ interaction and then makes decisions by recursive modeling. We address the deficiencies of the loosely coupled framework and the problem of mispredictions in Xiang’s architecture. The solution is based on Lawniczak’s Architecture for generic cognitive agents and an enhanced model of Xiang’s Recursive Modeling Method for coordinated decision-making in multi-agent situations. We instruct the cognitive agent to learn about other agents from past mispredictions and then consider its best choice. The feedback module is incorporated so agents can learn to maximize their joint expected reward. The model filters the mispredictions and evaluates the error rate. We compare the enhanced method with the Recursive Modeling Method. The results show that mispredictions are corrected from 33% to 10.9% and errors in perception are reduced from 22% to 0.097%, as the system progresses. Overall, the approach demonstrates superior performance. It significantly lowers the rate of mispredictions about other agents’ actions and takes 30% to 42% less time and 55.4 % fewer moves than RMM.","2169-3536","","10.1109/ACCESS.2023.3309417","Foundation University, Islamabad; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10232974","Cognitive agent;multi-agent system;recursive reasoning;agent reasoning;agent-oriented methodologies;loosely coupled framework;decision making;MAS communication;agent coordination;recursive modeling method","Decision making;Multi-agent systems;Computational modeling;Target tracking;Computer architecture;Cognition;Task analysis;Recursive estimation;Agent-based modeling","","","","45","CCBYNCND","28 Aug 2023","","","IEEE","IEEE Journals"
"Development of Deep-Learning-Based Autonomous Agents for Low-Speed Maneuvering in Unity","R. Berta; L. Lazzaroni; A. Capello; M. Cossu; L. Forneris; A. Pighetti; F. Bellotti","Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy",Journal of Intelligent and Connected Vehicles,"26 Sep 2024","2024","7","3","229","244","This study provides a systematic analysis of the resource-consuming training of deep reinforcement-learning (DRL) agents for simulated low-speed automated driving (AD). In Unity, this study established two case studies: garage parking and navigating an obstacle-dense area. Our analysis involves training a path-planning agent with real-time-only sensor information. This study addresses research questions insufficiently covered in the literature, exploring curriculum learning (CL), agent generalization (knowledge transfer), computation distribution (CPU vs. GPU), and mapless navigation. CL proved necessary for the garage scenario and beneficial for obstacle avoidance. It involved adjustments at different stages, including terminal conditions, environment complexity, and reward function hyperparameters, guided by their evolution in multiple training attempts. Fine-tuning the simulation tick and decision period parameters was crucial for effective training. The abstraction of high-level concepts (e.g., obstacle avoidance) necessitates training the agent in sufficiently complex environments in terms of the number of obstacles. While blogs and forums discuss training machine learning models in Unity, a lack of scientific articles on DRL agents for AD persists. However, since agent development requires considerable training time and difficult procedures, there is a growing need to support such research through scientific means. In addition to our findings, we contribute to the R&D community by providing our environment with open sources.","2399-9802","","10.26599/JICV.2023.9210039","CRF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10695161","automated driving;autonomous agents;deep reinforcement learning;curriculum learning;modeling and simulation","Training;Systematics;Connected vehicles;Navigation;Graphics processing units;Machine learning;Complexity theory","","10","","87","","26 Sep 2024","","","TUP","TUP Journals"
"A Multi-Layered AI-Driven Cybersecurity Architecture: Integrating Entropy Analytics, Fuzzy Reasoning, Game Theory, and Multi-Agent Reinforcement Learning for Adaptive Threat Defense","E. F. Siddiqui; M. Haleem; S. F. Ahmad; A. Salhi; A. T. Zamani; N. Varish","Department of Computer Science, Era University, Lucknow, Uttar Pradesh, India; Department of Computer Science, Era University, Lucknow, Uttar Pradesh, India; Department of Artificial Intelligence and Data Science, GITAM (Deemed to be) University, Hyderabad Campus, Hyderabad, Telangana, India; Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, P.O. Box 84428, Riyadh, Saudi Arabia; Department of Computer Science, Faculty of Science, Northern Border University, Arar, Saudi Arabia; Department of Computer Science and Engineering, GITAM (Deemed to be) University, Hyderabad Campus, Hyderabad, Telangana, India",IEEE Access,"3 Oct 2025","2025","13","","170235","170257","In the face of increasingly sophisticated cyberattacks, including adaptive adversaries and stealthy anomalies, key features of defense mechanisms should be effective, interpretable, and theoretically rooted. Conventional intrusion detection systems are typically based on a single-paradigm machine learning model which can be effective (because it is optimized for conditions), but fail in generalizability and falling back on an explanation of its prediction. This paper outlines a multi-layered AI-enabled cyber defense framework that integrates entropy analytics, fuzzy inference, game-theoretic defense, and multi-agent reinforcement learning (MARL) inside a closed-loop adaptive architecture. In its simplest form, the novelty of the paper is that, four functional paradigms - uncertainty quantification, interpretability, strategic adversarial thinking, and live policy adaptation - are placed into a single coherent system. The framework operates as sequential and feedback salients - entropy analytics quantify the uncertainty in are states, fuzzy inference end maps the uncertainty into qualitative decision rules, game theory shapes defender - attacker towards equilibrium strategies, and MARL dynamically updates those strategies for convergence and long term adaptation. The empirical work on appropriate benchmark intrusion detection datasets consistently outperformed baseline systems including the DDN, Fed-ID, AG-IDS, DL-FL systems producing a 6-12% increase in detection accuracy, lower false positive rates from non-intrusions, and a faster convergence, with adversarial examples across multiple epochs. Also, practical case studies reveal a level of improved explainability in threat classification and anomaly detection, which equates to practical interpretability for security analysts from the framework. The major contributions of the work are threefold: 1) an integrated multi-layered AI-based cybersecurity framework, 2) theoretical robustness results in bounded adversarial models, and 3) performance and interpretability form the systematic empirical evaluations over multiple datasets.","2169-3536","","10.1109/ACCESS.2025.3610526","Princess Nourah bint Abdulrahman University Researchers Supporting Project(grant numbers:PNURSP2025R909); Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11165264","Adversarial cybersecurity;entropy-based detection;fuzzy inference;game-theoretic defense;multi-agent reinforcement learning","Security;Artificial intelligence;Accuracy;Computer crime;Computer security;Reinforcement learning;Fuzzy logic;Entropy;Adaptation models;Real-time systems","","","","64","CCBY","16 Sep 2025","","","IEEE","IEEE Journals"
"Federated Reinforcement Learning Acceleration Method for Precise Control of Multiple Devices","H. -K. Lim; J. -B. Kim; I. Ullah; J. -S. Heo; Y. -H. Han","Department of Interdisciplinary Program in Creative Engineering, Korea University of Technology and Education, Cheonan, South Korea; Department of Computer Science Engineering, Korea University of Technology and Education, Cheonan, South Korea; Advanced Technology Research Center, Korea University of Technology and Education, Cheonan, South Korea; Department of Interdisciplinary Program in Creative Engineering, Korea University of Technology and Education, Cheonan, South Korea; Department of Computer Science Engineering, Korea University of Technology and Education, Cheonan, South Korea",IEEE Access,"27 May 2021","2021","9","","76296","76306","Nowadays, Reinforcement Learning (RL) is applied to various real-world tasks and attracts much attention in the fields of games, robotics, and autonomous driving. It is very challenging and devices overwhelming to directly apply RL to real-world environments. Due to the reality gap simulated environment does not match perfectly to the real-world scenario and additional learning cannot be performed. Therefore, an efficient approach is required for RL to find an optimal control policy and get better learning efficacy. In this paper, we propose federated reinforcement learning based on multi agent environment which applying a new federation policy. The new federation policy allows multi agents to perform learning and share their learning experiences with each other e.g., gradient and model parameters to increase their learning level. The Actor-Critic PPO algorithm is used with four types of RL simulation environments, OpenAI Gym's CartPole, MoutainCar, Acrobot, and Pendulum. In addition, we did real experiments with multiple Rotary Inverted Pendulum (RIP) to evaluate and compare the learning efficiency of the proposed scheme with both environments.","2169-3536","","10.1109/ACCESS.2021.3083087","Basic Science Research Program through the National Research Foundation of Korea (NRF) by the Ministry of Education(grant numbers:2018R1A6A1A03025526,NRF- 2020R1A6A3A13073735); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439484","Federated reinforcement learning;multi-agent;transfer learning;gradient sharing","Reinforcement learning;Transfer learning;Training;Performance evaluation;Games;Systems architecture;Servers","","19","","33","CCBY","24 May 2021","","","IEEE","IEEE Journals"
"Herbguard: An Ensemble Deep Learning Framework With Efficientnet and Vision Transformers for Fine-Grained Classification of Medicinal and Poisonous Plants","A. Baskota; S. Ghimire; A. Ghimire; P. Baskaran","School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology, Vellore, Tamil Nadu, India",IEEE Access,"23 Oct 2025","2025","13","","179333","179350","Classifying whether a plant is herbal or poisonous is a significant challenge for trekkers, hikers, and nature enthusiasts, particularly in remote areas where several unfamiliar plant species are encountered. Consumption or even close contact with some poisonous plant species may lead to serious health risks, highlighting the need for an intelligent and real-time classification system. In this study, we propose an approach based on deep-learning to classify plants into several species under herbal and poisonous categories. The system employs Convolutional Neural Network (CNN) based EfficientNetV2-S, designed for local feature extraction, trained on segmented images, and transformer-based ViT-Tiny, capable of capturing global dependencies in images, fine-tuned on unsegmented raw images. Both models are trained using a two-stage fine-tuning strategy with label smoothing, MixUp, and CutMix augmentations. Preprocessing steps include CLAHE-based contrast enhancement, HSV masking and GrabCut segmentation, that are applied to training images to focus on relevant plant regions. The models are evaluated on 48 different plant species, consisting of 40 herbal and 8 poisonous species, ultimately achieving species-level accuracies of 95.86% (EfficientNet) and 96.69% (ViT) on the validation dataset. A soft-voting ensemble of the two models further improves species-level accuracy to 97.10% upon validation and 98.43% upon testing, while category-level accuracy remains consistently above 99.7% for all the models. These results demonstrate that combining convolutional and transformer-based approaches leads to a robust, highly accurate classification system that is capable of distinguishing varieties of medicinal and poisonous plants, offering a practical tool for safe trekking, biodiversity monitoring, and herbal medicine research.","2169-3536","","10.1109/ACCESS.2025.3622382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11205506","Plant classification;herbal and poisonous plants;deep learning;convolutional neural network (CNN);EfficientNet;vision transformer;image segmentation;data augmentation;soft-voting ensemble","Accuracy;Transformers;Convolutional neural networks;Feature extraction;Medicinal plants;Biological system modeling;Image segmentation;Deep learning;Computer vision;Computational modeling","","","","23","CCBYNCND","16 Oct 2025","","","IEEE","IEEE Journals"
"Strategic Implementation of Super-Agents in Heterogeneous Multi-Agent Training for Advanced Military Simulation Adaptability","H. O. Altun; H. Furkan Ceran; K. Kutay Metın; T. Erol; E. Fişne","Institute for Data Science and Artificial Intelligence, Boğaziçi University, Istanbul, Türkiye; HAVELSAN, Çankaya, Ankara, Türkiye; HAVELSAN, Çankaya, Ankara, Türkiye; HAVELSAN, Çankaya, Ankara, Türkiye; Institute for Data Science and Artificial Intelligence, Boğaziçi University, Istanbul, Türkiye",IEEE Access,"6 Jun 2025","2025","13","","96544","96563","This study focuses on the application of reinforcement learning in tactical military simulation environments involving heterogeneous multi-agent systems. Optimizing Heterogeneous Multi-Agent Training (HMAT) through scenario-specific adjustments to the Proximal Policy Optimization (PPO) algorithm, we tackle the complexity of tactical simulations. Utilizing an advanced simulation platform, a diverse range of Reinforcement Learning (RL) agents are rigorously trained across various combat scenarios. A ‘super-agent’, an Artificial Intelligence (AI) orchestrator for multi-agent systems, marks a significant advancement in collaborative AI, enhancing operational performance. Comparative analysis highlights the strengths of both traditional RL approaches and HMAT in a unified computational framework. While independent learning agents excel in predictable environments with fast training capabilities, HMAT stands out in dynamic scenarios for its adaptability and superior performance. The integration of HMAT with ‘super-agents’ is shown to markedly improve the fidelity and adaptive capacity of military simulations. Experimental results demonstrate that our fine-tuned super-agent framework achieves up to 92% mission success rate, outperforming scenario-specific baselines by 15–20% in complex Suppression of Enemy Air Defenses (SEAD) and air-to-ground tasks.These enhancements have far-reaching implications, potentially revolutionizing strategic military training and operational planning and underscoring AI’s critical role in modern defense strategies.","2169-3536","","10.1109/ACCESS.2025.3573419","HAVELSAN’s Education Technologies Department; Forces in Virtual Environments Machine Learning (FIVE-ML) project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11015435","Advanced combat simulations;AI-driven tactical decision making;heterogeneous multi-agent systems;military simulation training;reinforcement learning;proximal policy optimization;super-agents and tactical scenario analysis","Training;Artificial intelligence;Scalability;Adaptation models;Decision making;Complexity theory;Multi-agent systems;Military computing;Heuristic algorithms;Computer architecture","","","","46","CCBYNCND","26 May 2025","","","IEEE","IEEE Journals"
"UAV cooperative air combat maneuver decision based on multi-agent reinforcement learning","Z. Jiandong; Y. Qiming; S. Guoqing; L. Yi; W. Yong","School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; Shenyang Aircraft Design Institute, Shenyang, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China",Journal of Systems Engineering and Electronics,"12 Jan 2022","2021","32","6","1421","1438","In order to improve the autonomous ability of unmanned aerial vehicles (UAV) to implement air combat mission, many artificial intelligence-based autonomous air combat maneuver decision-making studies have been carried out, but these studies are often aimed at individual decision-making in 1v1 scenarios which rarely happen in actual air combat. Based on the research of the 1v1 autonomous air combat maneuver decision, this paper builds a multi-UAV cooperative air combat maneuver decision model based on multi-agent reinforcement learning. Firstly, a bidirectional recurrent neural network (BRNN) is used to achieve communication between UAV individuals, and the multi-UAV cooperative air combat maneuver decision model under the actor-critic architecture is established. Secondly, through combining with target allocation and air combat situation assessment, the tactical goal of the formation is merged with the reinforcement learning goal of every UAV, and a cooperative tactical maneuver policy is generated. The simulation results prove that the multi-UAV cooperative air combat maneuver decision model established in this paper can obtain the cooperative maneuver policy through reinforcement learning, the cooperative maneuver policy can guide UAVs to obtain the overall situational advantage and defeat the opponents under tactical cooperation.","1004-4132","","10.23919/JSEE.2021.000121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9679711","decision-making;air combat maneuver;cooperative air combat;reinforcement learning;recurrent neural network","Atmospheric modeling;Reinforcement learning;Autonomous aerial vehicles;Weapons;Recurrent neural networks;Decision making;Target tracking","","82","","","","12 Jan 2022","","","BIAI","BIAI Journals"
